---
title: "（Swift）機械学習を利用して音声ファイルを分類する"
emoji: "🗂"
type: "tech"
topics: [Apple, Swift, "Core ML", 機械学習]
published: true
---
## はじめに

この記事は機械学習の初心者に向けた記事です。Appleの機械学習フレームワークCore MLを利用して音声ファイルを識別する手順を説明します。例えば人の声が記録されていたら「`speech`」、音楽が記録されていたら「`music`」が識別の結果になります。

記事を読み進める上で機械学習の知識は不要です。訓練済みモデルを利用するだけならば、簡単に機械学習をアプリに組み込めるのだと実感してもらうのが記事の目的になります。

「ちょっと待て、モデルって何だ？」と思いましたね。オライリー・ジャパンから発売中の書籍「[ディープラーニング実践ガイド](https://www.oreilly.co.jp/books/9784814400287/)」から引用しましょう。

> 簡単に言ってしまうなら、モデルとは関数です。1つまたは複数の入力を受け取り、出力を1つ返します。入力されるデータは、テキストや画像、音声、動画などさまざまです。出力されるのは何らかの予測の結果です。
> 
> 1章 人工知能の概観より引用

どうです、簡単そうでしょう？

Appleは訓練済みモデルを提供しているので、それを利用します。謎めいた数式が登場することは一切ありませんので、ご安心ください。

## 環境

記事の投稿にあたり、以下の環境で動作確認しました。

- MacBook Pro 2021 M1PRO
- macOS Monterey 12.6.7
- Apple Swift 5.7

## 実装

まずは以下のコードを`main.swift`として保存しましょう。このコードが何をしているのかは後ほど解説します。

```swift
import SoundAnalysis

class ResultsObserver: NSObject, SNResultsObserving {
  var results: [SNClassificationResult] = []

  func request(_ request: SNRequest, didProduce result: SNResult) {
    guard let result = result as? SNClassificationResult else {
      return
    }

    self.results.append(result)
  }
  func request(_ request: SNRequest, didFailWithError error: Error) {
    print("The analysis failed: \(error.localizedDescription)")
  }
  func requestDidComplete(_ request: SNRequest) {
    print("The request completed successfully!")
  }
}

func run() {
  if CommandLine.arguments.count < 2 {
    return
  }

  let audioFileURL = URL(fileURLWithPath: CommandLine.arguments[1])
  let resultsObserver = ResultsObserver()

  do {
    let classifySoundRequest = try SNClassifySoundRequest(classifierIdentifier: .version1)

    classifySoundRequest.windowDuration = CMTimeMake(value: 2, timescale: 1)
    classifySoundRequest.overlapFactor = 0.5

    let audioFileAnalyzer = try SNAudioFileAnalyzer(url: audioFileURL)

    try audioFileAnalyzer.add(classifySoundRequest, withObserver: resultsObserver)

    audioFileAnalyzer.analyze()
  } catch let error {
    print(error)
  }
  if resultsObserver.results.isEmpty {
    print("Oops! The input file might be too short.")
  }

  print("\nTime\tIdentifier\tConfidence")

  for result in resultsObserver.results {
    guard let classification = result.classifications.first else {
      continue
    }

    let timeInSeconds = String(format: "%.2f", result.timeRange.start.seconds)
    let percent = String(format: "%.1f%%", classification.confidence * 100.0)

    print("\(timeInSeconds)\t\(classification.identifier)\t\(percent)")
  }
}

run()
```

### 試運転

早速コードを実行してみましょう。ターミナルを開いて以下のコマンドを入力します。

```console
$ swift main.swift <音声ファイルのパス>
```

音声ファイルの形式は`afplay`コマンドで再生できるものであれば、何でも構いません。`.wav`や`.mp3`などが利用できます。

また、音声ファイルはモノラルとステレオどちらでも構いません。サンプリング周波数と量子化ビット数についても制限はありません。

後ほどコードの解説をしますが、今回は推論を1秒間隔で実行しています。推論に利用するバッファの長さは2秒としました。

つまり、0秒目から2秒目、1秒目から3秒目、2秒目から4秒目、...といった区間ごとに推論を行います。もちろん、推論の実行間隔やバッファの長さは変更可能です。

参考までに、いくつか実行例を示します。

### （実行例その1）話し声

まずは人の話し声が正しく識別されるか試します。

#### 音声ファイルの準備

ボイスメモアプリを開いて録音しても構わないのですが、あえて意地悪してみましょう。音声合成した声の識別です。

macOSには音声合成コマンド`say`が搭載されています。合成された音声ファイルがどのように識別されるか試します。以下のコマンドを実行してください。

```console
$ say -o voice.aiff -v Kyoko 'こんにちは、私の名前はKyokoです。日本語の音声をお届けします。'
```

-o`フラグで出力ファイル、`-v`フラグで話者名を指定しています。最後の引数には読み上げる文章を指定します。

上記のコマンドが失敗する場合、以下のコマンドを実行してください。利用可能な話者名の一覧が表示されます。

```console
$ say -v '?'
```

以下のように「`話者名 言語 #サンプル文章`」の形式で一覧が表示されます。

```console
Alex                en_US    # Most people recognize me by my voice.
Alice               it_IT    # Salve, mi chiamo Alice e sono una voce italiana.
Alva                sv_SE    # Hej, jag heter Alva. Jag är en svensk röst.
...
Kyoko               ja_JP    # こんにちは、私の名前はKyokoです。日本語の音声をお届けします。
...
Otoya               ja_JP    # こんにちは、私の名前はOtoyaです。日本語の音声をお届けします。
...
```

音声ファイルの生成コマンドはあくまで一例です。話者の指定は自由ですので、必ずしも`Kyoko`を指定する必要はありません。

#### 結果

以下のコマンドを実行します。

```console
$ swift main.swift voice.aiff
```

次のような結果が得られます。

```text
The request completed successfully!

Time	Identifier	Confidence
0.00	speech	95.1%
1.00	speech	95.2%
2.00	speech	94.3%
3.00	speech	91.2%
```

結果は3行目から表示されます。

- 1列目: 推論を実行した再生位置（秒）
- 2列目: 推論された音声の種類
- 3列目: 結果の確信度

素晴らしい結果です。正しく`speech`（話し声）として識別されています。確信度も90 %をこえています。

### （実行例その2）ピンクノイズ

次はピンクノイズがどのように識別されるか試します。ピンクノイズは「ゴー」という音色のノイズです。激しい雨や滝の流れに似た音色のノイズです。

#### 音声ファイルの準備

音声ファイルの作成にはsoxコマンドが便利です。Homebrewでインストールできます。

```console
$ brew install sox
```

以下のコマンドを実行するとピンクノイズが10秒間記録された音声ファイルが作成されます。フォーマットは44.1 kHz / 2 ch / 16 bit signed-integer PCMとしました。

```console
$ sox -n -r 44100 -c 2 -b 16 noise.wav synth pinknoise trim 0 10
```

#### 結果

以下のコマンドを実行します。

```console
$ swift main.swift noise.wav
```

次のような結果が得られます。

```text
The request completed successfully!

Time	Identifier	Confidence
0.00	music	15.7%
1.00	waterfall	23.4%
2.00	breathing	13.0%
3.00	mechanical_fan	12.6%
4.00	mechanical_fan	10.5%
5.00	waterfall	13.8%
6.00	water	28.2%
7.00	waterfall	15.5%
8.00	breathing	14.5%
```

結果は3行目から表示されます。

- 1列目: 推論を実行した再生位置（秒）
- 2列目: 推論された音声の種類
- 3列目: 結果の確信度

すべての確信度が30 %未満です。識別された音声の種類も一定ではありません。

- `music`（音楽） ... 15.7%
- `waterfall`（滝） ... 23.4%
- `breathing`（呼吸） ... 13.0%
- `mechanical_fan`（送風機） ... 12.6%
- `mechanical_fan`（送風機） ... 10.5%
- `waterfall`（滝） ... 13.8%
- `water`（水） ... 28.2%
- `waterfall`（滝） ... 15.5%
- `breathing`（呼吸） ... 14.5%

とはいえ、完全に的外れでもありません。たしかに「ゴー」という音色は送風機の音や呼吸するときの音と似ています。

実は、今回利用した訓練済みモデルに音声の種類として「`pink_noise`」は含まれていません。その前提を踏まえると、妥当な結果と言えるかもしれません。

## コードの解説

`main.swift`の実装を解説します。処理の大まかな流れは以下になります。

1. 分類機のモデルを選ぶ
2. 推論の実行間隔を設定する
3. 推論する

### 1. 分類機のモデルを選ぶ

まずはモデルの選択です。30行目の`SNClassifySoundRequest(classifierIdentifier: .version1)`が該当します。

将来的にAppleがモデルを追加するかもしれませんが、今のところ音声の分類気として訓練されたモデルは`.version1`の1種類のみです。

もちろん、独自に訓練したモデルを利用できます。Appleの機械学習フレームワークはは`.mlmodel`形式の訓練済みモデルを読み込むことができます。また、Tensorflowで学習したモデルを`.mlmodel`に変換したり、XcodeのCreate MLアプリで直接`.mlmodel`を作成したりできます。

モデルの訓練についてはMacBookやiPhone上で行うことができます。ただし、完全に新規のモデルではなく、訓練済みモデルを利用した連合学習を行うことになります。

## 2. 推論の実行間隔を設定する

`main.swift`の32・33行目が該当します。

```swift
classifySoundRequest.windowDuration = CMTimeMake(value: 2, timescale: 1)
classifySoundRequest.overlapFactor = 0.5
```

上記の例では音声バッファの長さ（`windowDuration`）を2秒に指定しています。サンプリング周波数が44.1 kHzであれば88200サンプルが分析の対象になります。

次の行はオーバーラップの割合を指定しています。`windowDuration`というプロパティ名が表しているとおり、窓関数によって分析対象のバッファは両端が少し削られます。

つまり、バッファの長さとして2秒を指定しても実際に分析されるのは0.1秒目から1.9秒目までかもしれません。そのような分析の取りこぼしを防ぐため、オーバーラップの割合を指定します。

上記の例については、`overlapFactor = 0.5`ですから50 %のオーバーラップを意味します。2秒の50 %ですから1秒が推論の実行間隔になります。

なお、`windowDuration`の値が尊重されるとは限らない点に注意してください。例えばバッファの長さを2秒から200ミリ秒に変更したとします。推論の実行間隔は100ミリ秒になることを期待しますが、私の環境（M1PROを搭載したMacBook Pro 2021年モデル）の場合、500ミリ秒の感覚で推論が実行されました。

Apple Developerのドキュメントには言及がありませんが、ハードウェアの世代によって推論の実行間隔が上下するのかもしれません。アプリケーションに機械学習を組み込む際は可能な限り対象デバイスの実機テストをお勧めします。

さらに余談になりますが、`overlapFactor`の範囲は0.0以上1.0未満です。公式ドキュメントには「supports values in the range `[0.0, 1.0]`」という曖昧な表現がされていますが、`1.0`を指定するとランタイムエラーが発生します。

ところで、`overlapFactor`として`0.99999`のような1.0に近い値を指定すると高頻度で推論が実行されることになります。私の環境で試したところ、プログラムがハングしてしばらく応答なしになりました。特別な理由がない限り、overlapFactorは0.5で固定するのがお勧めです。

### 3. 推論する

`main.swift`の39行目が該当します。SNAudioFileAnalyzerのanalyzeインスタンスメソッドを呼び出すと推論が開始されます。

コードをシンプルにするため実装例では同期的に実行しましたが、analyzeには非同期のメソッドも定義されています。詳細についてはApple DeveloperのS[NAudioFileAnalyzerのドキュメント](https://developer.apple.com/documentation/soundanalysis/snaudiofileanalyzer)を参照してください。

## パフォーマンスについて

Apple製品にはANE（Apple Neural Engine）とよばれる機械学習のためのハードウェアアクセラレータが搭載されています。Core MLを利用する際はANEの性能を踏まえた検討が必要になります。iPhoneについてはiPhone Xから、Macについては2020年に発売されたARMアーキテクチャのモデルからANEが搭載されています。

### 世代別iPhoneのFlops比較

参考までに、iPhoneに搭載されたANEのFlops性能を示します。以下は[Apple Machine Learning Research](https://machinelearning.apple.com/research/neural-engine-transformers)からの引用です。

| 機種名 | チップの世代 | Flops |
|:---|:---|:---|
| iPhone 13 Pro | A15 | 15.8 TFlops |
| iPhone 12 Pro | A14 | 11.66 TFlops |
| iPhone 11 Pro | A13 | 5.4 TFlops |
| iPhone XS | A12 |  5.4 TFlops |
\ iPhone X | A11 |  0.6 TFlops |

## 参考資料

1. [Classifying Sounds in an Audio File - Apple Developer](https://developer.apple.com/documentation/soundanalysis/classifying_sounds_in_an_audio_file)
2. [Classifying Sounds in an Audio Stream - Apple Developer](https://developer.apple.com/documentation/soundanalysis/classifying_sounds_in_an_audio_stream)
3. [SNClassifySoundRequest - Apple Developer](https://developer.apple.com/documentation/soundanalysis/snclassifysoundrequest)
4. [SNAudioFileAnalyzer - Apple Developer](https://developer.apple.com/documentation/soundanalysis/snaudiofileanalyzer)
5. [On-device APIs - Apple Developer](https://developer.apple.com/machine-learning/api/)
6. [Core ML - Apple Developer](https://developer.apple.com/documentation/coreml)
7. [Deploying Transformers on the Apple Neural Engine - Apple Machine Learning Research](https://machinelearning.apple.com/research/neural-engine-transformers)
