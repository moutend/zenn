---
title: "（Swift）MLImageClassifierを用いてiPhone上で画像分類モデルの訓練と推論を行う"
emoji: "📝"
type: "tech"
topics: [Apple, iOS, Swift, 機械学習]
published: false
---
## はじめに

Create MLのMLImageClassifierを用いてiPhone上で画像分類モデルの訓練と推論行うサンプルアプリを作成しました。iOS 15以上のデバイスで動作します。

https://github.com/moutend/TrainingMLImageClassifier

訓練済みモデルの`.mlmodel`ファイルをアプリに埋め込む例はよく見かけますが、モデルの訓練から推論までをiPhone上で完結させる例はあまり見かけません。オンデバイスな機械学習の一例として参考になれば幸いです。

## 不具合と思われる事象の共有

本題に進みたいところですが、先に実装の注意点をお伝えします。

### 1. 訓練が極端に遅くなる事象について

原因は不明ですが、最新のiOS 17でモデルの訓練が極端に遅くなる事象を確認しています。具体的にはiOS 15と比較して3倍ほど遅くなります。サンプルアプリを例にすると以下のような結果になりました。

- iOS 15とiPhone 13の組み合わせ→9.8秒
- iOS 17とiPhone 15 Proの組み合わせ→29.9秒

古いデバイスで性能の低下が生じるのであれば納得できます。しかし、最新のiOSと最高性能のデバイスの組み合わせで3倍ほど遅くなるのは不自然です。

原因の切り分けができず調査が難航していますが、ひとまず事象の共有でした。もし何かご存知の方がいましたら、コメントなどで教えていただけると助かります。

### 2. 訓練中のエラー「Must allow 2048-element vector as output」について

このエラーは`MLImageClassifier.FeatureExtractorType.scenePrint(revision: nil)`のように、リビジョン番号に`nil`を指定すると発生します。おそらくAppleの実装ミスです。

Apple Developerのドキュメントによると、リビジョン番号として`nil`が指定された場合は最新の番号が指定されたものとして扱うと説明されています。

> The sceneprint version. The supported versions include 1 and 2. If nil defaults to the latest version.
> 
> 引用元 https://developer.apple.com/documentation/createml/mlimageclassifier/featureextractortype/sceneprint(revision:)

なお、このエラーが発生するのはiOS 17以降のみです。iOS 15では発生しません。詳細については実装の要点で改めて説明します。

## この記事で扱う内容について

MLImageClassifierの利用方法についてはサンプルアプリをご確認ください。`Logic/ImageTrainer.swift`が訓練、`Logic/ImageClassifier.swift`が推論の実装になります。

この記事では実装を進める上で留意するべきことや開発中に生じる疑問についてお答えします。そもそもCreate MLが専門知識なしで手軽に扱えるフレームワークですので、機械学習についての前提知識は不要です。

ただし、機械学習の入門を済ませておくと記事を早く読み進められるかと思います。例えば書籍「ゼロから作るDeep Learning」がおすすめです。

- [O'Reilly Japan - ゼロから作るDeep Learning](https://www.oreilly.co.jp/books/9784873117584/)

## 訓練から推論までの流れ

まずはCreate MLを利用して画像分類器を作成・利用する流れを説明します。手順は以下の3ステップです。

1. 訓練データを用意します。具体的にはFileManagerで任意の場所にディレクトリを作成し、その中に画像ファイルを保存します。ディレクトリの名前が分類ラベルの名前になります。
2. 諸々のパラメータを設定して訓練を開始します。訓練が完了すると`.mlmodel`ファイルが作成されます。
3. 作成した`.mlmodel`ファイルをコンパイルします。その後、コンパイルされたモデルを読み込むと推論が可能な状態になります。

お手軽に使えて便利そうですね。しかし、この時点でいくつか疑問が生まれます。

- iPhoneはストレージやメモリなどリソースが限られている。訓練データのサイズや分類ラベルの個数に制限はあるのか？
- 訓練にはどれくらい時間がかかるのか？訓練中のリソース消費はどれくらいか？
- MacとiPhoneで訓練の内容は異なるのか？同じパラメータを指定して訓練した場合、最終的に作成される`.mlmodel`ファイルに違いはあるのか？
- モデルのコンパイルはどれくらい時間がかかるのか？コンパイルされたモデルの容量はどれくらいか？
- 推論にはどれくらい時間がかかるのか？推論のたびに都度コンパイルが必要なのか？コンパイルされたモデルを別のデバイスで再利用できるのか？

上記の疑問については記事の後半でお答えします。

## 実装の要点

ここからは実装の要点を説明します。項目は以下の5つあります。

1. 実質的にロジスティック回帰モデルしか選択肢がない
2. ModelParametersのデフォルト値に注意
3. MLTrainingSessionParametersのデフォルト値に注意
4. MLJobのcheckpointsプロパティで訓練の進捗を取得するにはMLImageClassifierの`train()`にsessionParameters引数を指定する必要がある。
5. MLJobのresultプロパティで訓練が完了した際の結果を受け取るにはAnyCancellableインスタンスを保持しておく必要がある。

### 1. 実質的にロジスティック回帰モデルしか選択肢がない

画像分類に利用できるモデルは実質的にロジスティック回帰モデルのみです。Apple Developerドキュメントから引用します。

> [MLImageClassifier.ModelParameters.ClassifierType - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct/classifiertype)
> 
> `case logisticRegressor` -- Logistic regression is a statistical model that classifies input feature vector into different categories.
> `case multilayerPerceptron(layerSizes: [Int])` -- Multilayer perceptron, layerSizes holds a list of positive integers that represent the number of hidden units in each layer. An additional fully connected layer with a Softmax activation output will be added that maps to probabilities of sound categories.

現状のCreate MLは2つの選択肢があり、1番目の`logisticRegressor`がロジスティック回帰モデルです。2番目の`multilayerPerceptron`がニューラルネットワークを利用したモデルです。しかし、2番目の説明を最後まで読むと音声を分類するため、と書かれています。

つまり、2番目の選択肢は画像処理でよく使われる畳み込みニューラルネットワークのようなモデルではなく、文字通りレイヤーを重ねただけの素朴なニューラルネットワークです。何にでも使えるので、とりあえず選択肢として含まれているようです。

余談になりますが、書籍「ゼロから作るDeep Learning」では素朴な2層のニューラルネットワークを用いてMNIST画像の分類気を実装する例が登場します。それなりに機能することが確認できておもしろい実験です。

### 2. ModelParametersのデフォルト値に注意

MLImageClassifier.ModelParametersについての話です。まずは`init()`の定義を確認しましょう。

```swift
init(
  validation: MLImageClassifier.ModelParameters.ValidationData = __Defaults.validation,
  maxIterations: Int = __Defaults.maximumIterations,
  augmentation: MLImageClassifier.ImageAugmentationOptions,
  algorithm: MLImageClassifier.ModelParameters.ModelAlgorithmType = __Defaults.algorithm
)
```

引数のデフォルト値は次のとおりです。

- validation: `.split(strategy: .automatic)`
- maxIterations: `25`
- augmentation: デフォルト値なし
- algorithm: `.transferLearning(featureExtractor: .scenePrint(revision: 1), classifier: .logisticRegressor)`

それぞれの引数の役割は次のとおりです。

- validation: 機械学習では汎化能力を獲得させるため、訓練データとテストデータを分割する必要があります。validationのデフォルト値は訓練データからテストデータを自動的に良い塩梅で分割することを指示します。通常はデフォルト値のままで問題ありません。
- maxIterations: 訓練のイテレーションを繰り返す回数の最大値を意味します。25という数値には特に根拠がなく、おそらく仮の値として置かれているだけです。この値は各自で調整する必要があります。
- augmentation: 訓練データの水増しを行うか指示します。画像のぼかしや傾きを加えることを指示できますが、どれくらいボケを加えるのかや傾斜の角度など細かな指示はできません。
- algorithm: モデルの訓練に使用するアルゴリズムを指定します。

scenePrintのrevision番号のデフォルト値は`1`になっていますが、現在は`2`が利用可能です。なおバージョン番号ではなく`nil`を指定すると常に最新のバージョンを利用することを指示できます。

以上を踏まえた上でパラメータを作成すると、例えば次のようになります。

```swift
let parameters = MLImageClassifier.ModelParameters(
  validation: .split(strategy: .automatic),
  maxIterations: 50,
  augmentation: [],
  algorithm: .transferLearning(
    featureExtractor: .scenePrint(revision: nil),
    classifier: .logisticRegressor
  )
)
```

### 3. MLTrainingSessionParametersのデフォルト値に注意

MLImageClassifierのクラスメソッド`train()`についての話です。シグネチャは次の通りです。

```swift
static func train(
  trainingData: MLImageClassifier.DataSource,
  parameters: MLImageClassifier.ModelParameters = ModelParameters(
      validation: .split(strategy: .automatic),
      augmentation: [],
      algorithm: .transferLearning(
        featureExtractor: .scenePrint(revision: 1),
        classifier: .logisticRegressor
      )
    ),
  sessionParameters: MLTrainingSessionParameters = _defaultSessionParameters
) throws -> MLJob<MLImageClassifier>
```

ここで引数`sessionParameters`に注目してください。デフォルト値が設定されていますが、具体的にどのような値なのか説明がありません。

続いて`MLTrainingSessionParameters`の`init()`がどのように定義されているか確認してみましょう。

```swift
init(
  sessionDirectory: URL? = nil,
  reportInterval: Int = 5,
  checkpointInterval: Int = 10,
  iterations: Int = 1000
)
```

さて、`_defaultSessionParameters`とMLTrainingSessionParametersの`init()`が返す値は同じなのでしょうか？

答えはNOです。両者のデフォルト値は異なります。

どのようなアプリを実装するかに依存しますが、おそらく`init()`が返すデフォルト値は通常のユースケースでは頻度が細かすぎる上にイテレーションの回数が多すぎます。特に理由がなければ`train()`の`sessionParameters`は省略して`_defaultSessionParameters`を使うのがおすすめです。

### 4. MLJobのcheckpointsプロパティで訓練の進捗を取得するにはMLImageClassifierの`train()`にsessionParameters引数を指定する必要がある

MLImageClassifierの`train()`で訓練を開始した場合の話です。`sessionParameters`は省略するのがおすすめと説明しましたが、そうすると訓練の進捗が取得できなくなります。訓練に相当な時間が必要になると見込まれる場合は`sessionParameters`を設定して、MLJobのcheckpointsで進捗を受け取れるようにしましょう。

なお`sessionParameters`でセッションを保存するように指示する場合、訓練途中に中断するケースの考慮が必要になります。具体的にはMLTrainingSessionParametersの`sessionDirectory`で指示したディレクトリにセッションが記録されたファイルが残ります。

この状態で再度MLImageClassifierの`train()`を実行するとエラーが発生します。途中からではなく訓練を最初から再実行したい場合はセッションが記録されたファイルを削除する必要があります。

### 5. MLJobのresultプロパティで訓練が完了した際の結果を受け取るにはAnyCancellableインスタンスを保持しておく必要がある

Create MLとは関係のない話題ですが、AnyPublisher型のプロパティから値を取得する場合の話です。PublisherはCombineフレームワークが提供するバインディング機構です。詳細については以下のドキュメントを参照してください。

- [Combine - Apple Developer Documentation](https://developer.apple.com/documentation/combine)
- [Publisher - Apple Developer Documentation](https://developer.apple.com/documentation/combine/publisher)
- [AnyPublisher - Apple Developer Documentation](https://developer.apple.com/documentation/Combine/AnyPublisher)
- [AnyCancellable - Apple Developer Documentation](https://developer.apple.com/documentation/combine/anycancellable)

なお、MLJobの訓練を中断したい場合はMLJobの`cancel()`メソッドを実行してください。

実装例

```swift
// 動作する
self.resultCancellable = self.job?.result.sink(
  receiveCompletion: { [weak self] completion in
    // ...
  },
  receiveValue: { [weak self] classifier in
    // ...
  }
)

// 動作しない
self.job?.result.sink(
  receiveCompletion: { [weak self] completion in
    // ...
  },
  receiveValue: { [weak self] classifier in
    // ...
  }
)
```

## よくある質問と回答

### 1. 

## 参考資料

1. [MLJob - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mljob)
2. [MLImageClassifier.ModelParameters - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct)
3. [MLImageClassifier.FeatureExtractorType - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/featureextractortype)
4. [MLImageClassifier.ModelParameters.ModelAlgorithmType - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct/modelalgorithmtype)
5. [iOSでのCreateML / CoreMLを用いたオンデバイス機械学習 - エニグモ開発者ブログ](https://tech.enigmo.co.jp/entry/2022/12/20/070000)
