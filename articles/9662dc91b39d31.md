---
title: "（Swift）Visionフレームワークを用いて画像から日本語のテキストを抽出する"
emoji: "📑"
type: "tech"
topics: [Apple, iOS, Swift, OCR]
published: false
---
## はじめに

日本語OCRの話題です。iOS 16以降、Visionフレームワークが提供するテキスト抽出機能の言語として日本語が利用可能になりました。この記事では実装例を示した後に、日本語テキスト抽出の注意点をまとめます。

## 環境

以下の環境にて動作確認を行いました。

- iOS 17.5.1
- Xcode Version 15.4 (15F31d)

## 補足

実装例に進む前に2点補足します。

### 日本語が利用可能か判定する

新規にアプリを制作する場合はターゲットをiOS 16以降に設定すれば確実に日本語が利用できます。あるいはディレクティブを指定してiOSバージョンごとに実装を分岐させる方法も考えられます。

一方、アプリ実行時に日本語が利用可能か判定したい場合もあります。VNRecognizeTextRequestのsupportedRecognitionLanguagesメソッドを使用すれば以下のように判定できます。

```swift
do {
  let request = VNRecognizeTextRequest()
  let languages = try request.supportedRecognitionLanguages()
  // languagesにはISO言語コードの文字列が格納される。日本語が利用可能な場合はja-JPが含まれる。
} catch {
  fatalError("Failed to get supported languages: \(error)")
}
```

### iOS 18以降のVisionフレームワークについて

Apple Developerドキュメントによると、iOS 18以降のVisionフレームワークが次のように変更されるそうです。

> Starting in iOS 18.0, the Vision framework provides a new Swift-only API. See Original Objective-C and Swift API to view the original API.
> 
> 引用元 https://developer.apple.com/documentation/vision

既存のAPIに影響はありません。この記事ではOriginal Objective-C and Swift APIとして提供されているAPIを前提に説明を進めます。

## 実装例

Apple Developerドキュメントに実装の詳しい説明があります。まずはそちらを参照してください。

[Recognizing Text in Images - Apple Developer Documentation](https://developer.apple.com/documentation/vision/recognizing-text-in-images)

参考までに実装例を示します。

```swift
import CoreImage
import Vision

class TextRecognizer {
  func perform(ciImage: CIImage) {
    let handler = VNImageRequestHandler(ciImage: ciImage)
    let request = VNRecognizeTextRequest(completionHandler: self.recognizeTextHandler)

    request.recognitionLanguages = ["ja-JP", "en-US"]

    do {
      try handler.perform([request])
    } catch {
      print("Unable to perform the requests: \(error).")
    }
  }
  func recognizeTextHandler(request: VNRequest, error: Error?) {
    guard let observations = request.results as? [VNRecognizedTextObservation] else {
      return
    }
    for observation in observations {
      guard let candidate = observation.topCandidates(1).first else {
        continue
      }

      let stringRange = candidate.string.startIndex..<candidate.string.endIndex

      guard let box = try? candidate.boundingBox(for: stringRange) else {
        continue
      }

      let width = box.bottomRight.x - box.topLeft.x
      let height = box.topLeft.y - box.bottomRight.y

      let info = String(
        format: "Confidence(%.1f)\tBox(x: %.2f, y: %.2f, width: %.2f, height: %.2f)",
        candidate.confidence, box.topLeft.x, box.topLeft.y, width, height
      )

      print("\(info)\t\(candidate.string)")
    }
  }
}
```

## 実装の考慮事項

### FastとAccurate

テキストの認識はFast（高速）とAccurate（高精度）の2種類があります。デフォルトではAccurateで認識されるため、以降はその前提で説明を続けます。

> VNRecognizeTextRequest uses the accurate path by default. To select the fast path, set the request’s recognitionLevel property to VNRequestTextRecognitionLevel.fast.
> 
> 引用元 https://developer.apple.com/documentation/vision/recognizing-text-in-images

### confidenceプロパティの値

VNRecognizedTextObservationのconfidenceプロパティ（認識結果の確信度）の値は0.0から1.0の範囲に正規化されています。

ドキュメントには記載されていませんが、confidenceの値は連続的に変化する値ではなく0.1刻みの値のようです。また、Accurateで試した限り、confidenceの値は0.3、0.5、1.0のいずれかでした。

## 参考資料

1. [Vision - Apple Developer Documentation](https://developer.apple.com/documentation/vision)
2. [Original Objective-C and Swift API - Apple Developer Documentation](https://developer.apple.com/documentation/vision/original-objective-c-and-swift-api)
3. [Recognizing Text in Images - Apple Developer Documentation](https://developer.apple.com/documentation/vision/recognizing-text-in-images)
