---
title: "PyTorchで書籍「ゼロから作るDeep Learning」のTwoLayerNetを実装する"
emoji: "🕌"
type: "tech"
topics: [PyTorch, Python, 機械学習]
published: false
---
## はじめに

書籍「ゼロから作るDeep Learning」を読み終えたので、次のステップとしてPyTorchの学習を進めています。まずは簡単な題材として、書籍の第4章で登場するTwoLayerNetを実装することにしました。

## 環境

記事の投稿にあたり、以下の環境で動作確認しました。

- macOS 12.7.1 Monterey
- Anaconda 23.7.4
- Python 3.11.5

## モデルの訓練と保存

```python
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor

def get_available_device():
	if torch.cuda.is_available() :
		return 'cuda'
	elif torch.backends.mps.is_available():
		return 'mps'
	else:
		return 'cpu'

class TwoLayerNet(nn.Module):
	def __init__(self, input_size, hidden_size, output_size):
		super().__init__()
		self.flatten = nn.Flatten()
		self.layers = nn.Sequential(
			nn.Linear(input_size, hidden_size),
			nn.ReLU(),
			nn.Linear(hidden_size, output_size)
			# nn.Softmax(dim=1)
		)
	def forward(self, x):
		x = self.flatten(x)
		logits = self.layers(x)
		return logits

# どのハードウェアで訓練を行うか選択する。macOS 12.3以降のバージョンであればMetal Performance Shaders（'mps'）が選択される。
device = get_available_device()

# モデルを作成する。
# 
# input_sizeには特徴量の次元を指定する。MNISTの画像は縦横28 pxの正方形なので28 * 28 = 784となる。
# hidden_sizeには隠れ層の重みの次元を指定する。この値は適当で構わないので、書籍に習って100を指定する。
# output_sizeには分類される数を指定する。MNISTのデータセットには0から9までの手書き数字10種類が含まれているため、output_sizeには10を指定する。
model = TwoLayerNet(input_size=784, hidden_size=100, output_size=10).to(device)

# 訓練データを読み込む。バッチのサイズは適当で構わないので、書籍に習って100を指定する。
batch_size = 100
training_datasets = datasets.MNIST(root="/tmp/torch/data", train=True, download=True, transform=ToTensor())
training_dataloader = DataLoader(training_datasets, batch_size=batch_size, shuffle=True)

# 損失関数として交差エントロピー誤差、オプティマイザとして確率的勾配降下法を利用する。Learning Rateなどのハイパーパラメータは適当で構わない。
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)

# エポックの数は適当なので訓練の進み具合に応じて増減させてください。
for epoch in range(25):
	running_loss = 0.0
	
	for i, data in enumerate(training_dataloader, 0):
		inputs, labels = data
		inputs, labels = inputs.to(device), labels.to(device)
		optimizer.zero_grad()
		outputs = model(inputs)
		loss = criterion(outputs, labels)
		loss.backward()
		optimizer.step()
		running_loss += loss.item()
		if i % 600 == 0:
			print(f'epoch={epoch + 1}, loss={running_loss:.5f}')
			running_loss = 0.0

torch.save(model.state_dict(), '/tmp/torch/mnist.pth')

print('Finished')
```

## モデルのテスト

```python
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor

def get_available_device():
	if torch.cuda.is_available() :
		return 'cuda'
	elif torch.backends.mps.is_available():
		return 'mps'
	else:
		return 'cpu'

class TwoLayerNet(nn.Module):
	def __init__(self, input_size, hidden_size, output_size):
		super().__init__()
		self.flatten = nn.Flatten()
		self.layers = nn.Sequential(
			nn.Linear(input_size, hidden_size),
			nn.ReLU(),
			nn.Linear(hidden_size, output_size)
			# nn.Softmax(dim=1)
		)
	def forward(self, x):
		x = self.flatten(x)
		logits = self.layers(x)
		return logits

device = get_available_device()
print(device)

model = TwoLayerNet(input_size=784, hidden_size=100, output_size=10).to(device)
model_path = './mnist.pth'
model.load_state_dict(torch.load(model_path))

batch_size = 100

test_datasets = datasets.MNIST(root="/tmp/torch/data", train=False, download=True, transform=ToTensor())
test_dataloader = DataLoader(test_datasets, batch_size=batch_size, shuffle=True)

correct = 0
total = 0

with torch.no_grad():
	for data in test_dataloader:
		inputs, labels = data
		inputs = inputs.to(device)
		labels = labels.to(device)
		outputs = model(inputs)
		_, predicted = torch.max(outputs.data, 1)
		total += labels.size(0)
		correct += (predicted == labels).sum().item()

print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')
```

## （おまけ）PyTorchのチュートリアル

## 参考資料

