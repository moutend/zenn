---
title: "（Swift）MLImageClassifierを用いてiPhone上で画像分類モデルの訓練と推論を行う"
emoji: "📝"
type: "tech"
topics: [Apple, iOS, Swift, 機械学習]
published: false
---
## はじめに

Create MLのMLImageClassifierを用いてiPhone上で画像分類モデルの訓練と推論行うサンプルアプリを作成しました。iOS 15以上のデバイスで動作します。

https://github.com/moutend/TrainingMLImageClassifier

訓練済みモデルの`.mlmodel`ファイルをアプリに埋め込む例はよく見かけますが、モデルの訓練から推論までをiPhone上で完結させる例はあまり見かけません。オンデバイスな機械学習の一例として参考になれば幸いです。

### この記事について

MLImageClassifierの利用方法についてはサンプルアプリをご確認ください。`Logic/ImageTrainer.swift`が訓練、`Logic/ImageClassifier.swift`が推論の実装になります。

この記事では実装を進める上で留意するべきことや開発中に生じる疑問についてお答えします。そもそもCreate MLが専門知識なしで手軽に扱えるフレームワークですので、機械学習についての前提知識は不要です。

ただし、機械学習の入門を済ませておくと記事を早く読み進められるかと思います。例えば書籍「ゼロから作るDeep Learning」がおすすめです。

- [O'Reilly Japan - ゼロから作るDeep Learning](https://www.oreilly.co.jp/books/9784873117584/)

### 訓練から推論までの流れ

まずはCreate MLを利用して画像分類器を作成・利用する流れを説明します。手順は以下の3ステップです。

1. 訓練データを用意します。具体的にはFileManagerで任意の場所にディレクトリを作成し、その中に画像ファイルを保存します。ディレクトリの名前が分類ラベルの名前になります。
2. 諸々のパラメータを設定して訓練を開始します。訓練が完了すると`.mlmodel`ファイルが作成されます。
3. 作成した`.mlmodel`ファイルをコンパイルします。その後、コンパイルされたモデルを読み込むと推論が可能な状態になります。

お手軽に使えて便利そうですね。しかし、この時点でいくつか疑問が生まれます。

- iPhoneはストレージやメモリなどリソースが限られている。訓練データのサイズや分類ラベルの個数に制限はあるのか？
- 訓練にはどれくらい時間がかかるのか？訓練中のリソース消費はどれくらいか？
- MacとiPhoneで訓練の内容は異なるのか？同じパラメータを指定して訓練した場合、最終的に作成される`.mlmodel`ファイルに違いはあるのか？
- モデルのコンパイルはどれくらい時間がかかるのか？コンパイルされたモデルの容量はどれくらいか？
- 推論のたびに都度コンパイルが必要なのか？コンパイルされたモデルを別のデバイスで再利用できるのか？

上記の疑問については後ほどお答えします。

## 実装の要点

ここからは実装の要点を説明します。

1. 実質的にロジスティック回帰モデルしか選択肢がない
2. ModelParametersのデフォルト値に注意
3. MLTrainingSessionParametersのデフォルト値に注意

### 1. 実質的にロジスティック回帰モデルしか選択肢がない

画像分類に利用できるモデルは実質的にロジスティック回帰モデルのみです。Apple Developerドキュメントから引用します。

> [MLImageClassifier.ModelParameters.ClassifierType - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct/classifiertype)
> 
> `case logisticRegressor` -- Logistic regression is a statistical model that classifies input feature vector into different categories.
> `case multilayerPerceptron(layerSizes: [Int])` -- Multilayer perceptron, layerSizes holds a list of positive integers that represent the number of hidden units in each layer. An additional fully connected layer with a Softmax activation output will be added that maps to probabilities of sound categories.

現状のCreate MLは2つの選択肢があり、1番目の`logisticRegressor`がロジスティック回帰モデルです。2番目の`multilayerPerceptron`がニューラルネットワークを利用したモデルです。しかし、2番目の説明を最後まで読むと音声を分類するため、と書かれています。

つまり、2番目の選択肢は画像処理でよく使われる畳み込みニューラルネットワークのようなモデルではなく、文字通りレイヤーを重ねただけの素朴なニューラルネットワークです。何にでも使えるので、とりあえず選択肢として含まれているようです。

余談になりますが、書籍「ゼロから作るDeep Learning」では素朴な2層のニューラルネットワークを用いてMNIST画像の分類気を実装する例が登場します。それなりに機能することが確認できておもしろい実験です。

### 2. ModelParametersのデフォルト値に注意

MLImageClassifier.ModelParametersについての話です。まずは`init()`の定義を確認しましょう。

```swift
init(
  validation: MLImageClassifier.ModelParameters.ValidationData = __Defaults.validation,
  maxIterations: Int = __Defaults.maximumIterations,
  augmentation: MLImageClassifier.ImageAugmentationOptions,
  algorithm: MLImageClassifier.ModelParameters.ModelAlgorithmType = __Defaults.algorithm
)
```

引数のデフォルト値は次のとおりです。

- validation: `.split(strategy: .automatic)`
- maxIterations: `25`
- augmentation: デフォルト値なし
- algorithm: `.transferLearning(featureExtractor: .scenePrint(revision: 1), classifier: .logisticRegressor)`

それぞれの引数の役割は次のとおりです。

- validation: 機械学習では汎化能力を獲得させるため、訓練データとテストデータを分割する必要があります。validationのデフォルト値は訓練データからテストデータを自動的に良い塩梅で分割することを指示します。通常はデフォルト値のままで問題ありません。
- maxIterations: 訓練のイテレーションを繰り返す回数の最大値を意味します。25という数値には特に根拠がなく、おそらく仮の値として置かれているだけです。この値は各自で調整する必要があります。
- augmentation: 訓練データの水増しを行うか指示します。画像のぼかしや傾きを加えることを指示できますが、どれくらいボケを加えるのかや傾斜の角度など細かな指示はできません。
- algorithm: モデルの訓練に使用するアルゴリズムを指定します。

scenePrintのrevision番号のデフォルト値は`1`になっていますが、現在は`2`が利用可能です。なおバージョン番号ではなく`nil`を指定すると常に最新のバージョンを利用することを指示できます。

以上を踏まえた上でパラメータを作成すると、例えば次のようになります。

```swift
let parameters = MLImageClassifier.ModelParameters(
  validation: .split(strategy: .automatic),
  maxIterations: 50,
  augmentation: [],
  algorithm: .transferLearning(
    featureExtractor: .scenePrint(revision: nil),
    classifier: .logisticRegressor
  )
)
```

### 3. MLTrainingSessionParametersのデフォルト値に注意

MLImageClassifierのクラスメソッド`train()`についての話です。シグネチャは次の通りです。

```swift
static func train(
  trainingData: MLImageClassifier.DataSource,
  parameters: MLImageClassifier.ModelParameters = ModelParameters(
      validation: .split(strategy: .automatic),
      augmentation: [],
      algorithm: .transferLearning(
        featureExtractor: .scenePrint(revision: 1),
        classifier: .logisticRegressor
      )
    ),
  sessionParameters: MLTrainingSessionParameters = _defaultSessionParameters
) throws -> MLJob<MLImageClassifier>
```

ここで引数`sessionParameters`に注目してください。デフォルト値が設定されていますが、具体的にどのような値なのか説明がありません。

続いて`MLTrainingSessionParameters`の`init()`がどのように定義されているか確認してみましょう。

```swift
init(
  sessionDirectory: URL? = nil,
  reportInterval: Int = 5,
  checkpointInterval: Int = 10,
  iterations: Int = 1000
)
```

さて、`_defaultSessionParameters`とMLTrainingSessionParametersの`init()`が返す値は同じなのでしょうか？

答えはNOです。両者のデフォルト値は異なります。

どのようなアプリを実装するかに依存しますが、おそらく`init()`が返すデフォルト値は通常のユースケースでは頻度が細かすぎる上にイテレーションの回数が多すぎます。特に理由がなければ`train()`の`sessionParameters`は省略して`_defaultSessionParameters`を使うのがおすすめです。

## 参考資料

1. [MLJob - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mljob)
2. [MLImageClassifier.ModelParameters - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct)
3. [MLImageClassifier.FeatureExtractorType - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/featureextractortype)
4. [MLImageClassifier.ModelParameters.ModelAlgorithmType - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct/modelalgorithmtype)
5. [iOSでのCreateML / CoreMLを用いたオンデバイス機械学習 - エニグモ開発者ブログ](https://tech.enigmo.co.jp/entry/2022/12/20/070000)
