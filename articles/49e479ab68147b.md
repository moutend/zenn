---
title: "（Swift）iPhoneでCreate MLのMLImageClassifierを用いて画像分類モデルの訓練と推論を行う"
emoji: "📝"
type: "tech"
topics: [Apple, iOS, Swift, CreateML, 機械学習]
published: false
---
## はじめに

iPhoneでCreate MLのMLImageClassifierを用いて画像分類モデルの訓練と推論行うサンプルアプリを作成しました。iOS 15以上のデバイスで動作します。

https://github.com/moutend/TrainingMLImageClassifier

訓練済みモデルの`.mlmodel`ファイルをアプリに埋め込む例はよく見かけますが、モデルの訓練から推論までをiPhone上で完結させる例はあまり見かけません。オンデバイスで処理する一例として参考になれば幸いです。

## 不具合と思われる事象

本題に進みたいところですが、不具合と思われる事象に遭遇したので共有します。実装の要点だけ知りたい場合、読み飛ばしていただいて構いません。

### 1. 訓練中のエラー「Must allow 2048-element vector as output」

iOS 17が搭載されたデバイスでモデルを訓練すると、Xcodeのコンソールに以下のエラーが表示されて処理が中断します。

```text
MLModelAsset: load failed with error Error Domain=com.apple.CoreML Code=0 "Must allow 2048-element vector as output" UserInfo={NSLocalizedDescription=Must allow 2048-element vector as output}
```

おそらくAppleの実装ミスが原因です。このエラーは`MLImageClassifier.FeatureExtractorType.scenePrint(revision: nil)`のように、リビジョン番号に`nil`を指定すると発生します。

Apple Developerのドキュメントによると、リビジョン番号として`nil`が指定された場合は最新の番号が指定されたものとして扱うそうです。

> The sceneprint version. The supported versions include 1 and 2. If nil defaults to the latest version.
> 
> 引用元 https://developer.apple.com/documentation/createml/mlimageclassifier/featureextractortype/sceneprint(revision:)

しかし、iOS 17では`nil`を指定すると上記のエラーが発生します。このエラーを回避するにはリビジョン番号として`1`か`2`を指定してください。

なお、このエラーはiOS 15では発生しません。詳細については実装の要点で改めて説明します。

### 2. 訓練が遅い

原因は不明ですが、iOS 17でモデルの訓練にかかる時間が遅くなる事象を確認しています。具体的にはiOS 15と比較して3倍ほど遅くなります。サンプルアプリを例にすると以下のような結果になりました。

- iOS 15とiPhone 13の組み合わせ→9.8秒
- iOS 17とiPhone 15 Proの組み合わせ→29.9秒
- iOS 17とiPhone SE 第2世代の組み合わせ→29.9秒

Appleのドキュメントによると、iPhone 13に搭載されているA15 Bionicチップの性能は15.8 TFLOPSです。iPhone 15 Proに搭載されているA17 ProチップのTFLOPS性能は公表されていませんが、A15世代と比較して2倍の性能だそうです。

従って、ハードウェアではなくiOSに原因がありそうです。Create MLフレームワークの内部的なパラメータが変更されて訓練が遅くなっているのかもしれません。なお推論にかかる時間はiPhone 15世代の方が高速です。

残念ながら解決策は見つけられていませんが、ひとまず事象の共有でした。もし何かご存知の方がいましたら、コメントなどで教えていただけたら幸いです。

## この記事について

Create MLのMLImageClassifierの使い方についてはサンプルアプリをご確認ください。`Logic/ImageTrainer.swift`が訓練、`Logic/ImageClassifier.swift`が推論の実装になります。

この記事では実装の要点や開発中に生じる疑問についてお答えします。Create MLが専門的な知識なしで手軽に扱えるフレームワークですので、記事を読み進めるための前提知識は不要です。

ただし、機械学習の入門を済ませておくと記事を早く読み進められるかと思います。例えば書籍「ゼロから作るDeep Learning」がおすすめです。隅から隅まで読み込む必要はなく、概要を把握するだけでも十分です。

- [O'Reilly Japan - ゼロから作るDeep Learning](https://www.oreilly.co.jp/books/9784873117584/)

## 訓練から推論までの流れ

それではCreate MLを利用して画像分類器を作成・利用する流れを説明します。手順は以下の3ステップです。

1. 訓練データを用意します。具体的にはFileManagerで任意の場所にディレクトリを作成し、その中に画像ファイルを保存します。ディレクトリの名前が分類ラベルの名前になります。
2. 諸々のパラメータを設定して訓練を開始します。訓練が完了すると`.mlmodel`ファイルが作成されます。
3. 作成した`.mlmodel`ファイルをコンパイルします。その後、コンパイルされたモデルを読み込むと推論が可能な状態になります。

Create MLのお手軽さが伝わったでしょうか。

ただし手軽だからといって、使いこなすのが簡単とは限りません。その点を補足していきます。

## 実装の要点

ここからは実装の要点を説明します。項目は以下の5つです。

1. 実質的にロジスティック回帰モデルしか選択肢がない
2. ModelParametersのデフォルト値に注意
3. MLTrainingSessionParametersのデフォルト値に注意
4. MLJobのcheckpointsプロパティで訓練の進捗を取得するにはMLImageClassifierの`train()`にsessionParameters引数を指定する必要がある。
5. MLJobのresultプロパティで訓練が完了した際の結果を受け取るにはAnyCancellableインスタンスを保持しておく必要がある。

### 1. 実質的にロジスティック回帰モデルしか選択肢がない

画像分類に利用できるモデルは実質的にロジスティック回帰モデルのみです。Apple Developerドキュメントから引用します。

> `case logisticRegressor` -- Logistic regression is a statistical model that classifies input feature vector into different categories.
> `case multilayerPerceptron(layerSizes: [Int])` -- Multilayer perceptron, layerSizes holds a list of positive integers that represent the number of hidden units in each layer. An additional fully connected layer with a Softmax activation output will be added that maps to probabilities of sound categories.
> 
> 引用元 https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct/classifiertype

ロジスティック回帰モデルは機械学習モデルの一種です。流行りのニューラルネットワーク登場以前からある、技術的に枯れた手法です。

現状のCreate MLは2つの選択肢があり、1番目の`logisticRegressor`がロジスティック回帰モデルです。2番目の`multilayerPerceptron`がニューラルネットワークを利用したモデルです。しかし、2番目の説明を最後まで読むと音声を分類するため、と書かれています。

つまり、2番目の選択肢は画像処理でよく使われる畳み込みニューラルネットワークなどではなく、文字通りレイヤーを重ねただけの素朴なニューラルネットワークです。何にでも使えるので、とりあえず選択肢として含まれているようです。

余談になりますが、書籍「ゼロから作るDeep Learning」では素朴な2層のニューラルネットワークを用いてMNIST画像の分類気を実装する例が登場します。それなりに機能することが確認できておもしろい実験です。

### 2. ModelParametersのデフォルト値に注意

MLImageClassifier.ModelParametersについての話題です。まずは`init()`の定義を確認しましょう。

```swift
init(
  validation: MLImageClassifier.ModelParameters.ValidationData = __Defaults.validation,
  maxIterations: Int = __Defaults.maximumIterations,
  augmentation: MLImageClassifier.ImageAugmentationOptions,
  algorithm: MLImageClassifier.ModelParameters.ModelAlgorithmType = __Defaults.algorithm
)
```

引数のデフォルト値は次のとおりです。

- validation: `.split(strategy: .automatic)`
- maxIterations: `25`
- augmentation: デフォルト値なし
- algorithm: `.transferLearning(featureExtractor: .scenePrint(revision: 1), classifier: .logisticRegressor)`

それぞれの引数の役割は次のとおりです。

- validation: 機械学習では汎化能力を獲得させるため、訓練データとテストデータを分割する必要があります。validationのデフォルト値は訓練データからテストデータを自動的に良い塩梅で分割することを指示します。通常はデフォルト値のままで問題ありません。
- maxIterations: 訓練のイテレーションを繰り返す回数の最大値を意味します。25という数値には特に根拠がなく、おそらく仮の値として置かれているだけです。この値は各自で調整する必要があります。
- augmentation: 訓練データの水増しを行うか指示します。画像のぼかしや傾きを加えることを指示できますが、どれくらいボケを加えるのかや傾斜の角度など細かな指示はできません。
- algorithm: モデルの訓練に使用するアルゴリズムを指定します。enumで選択肢が定義されていますが、今のところ利用できるのはこの選択肢のみです。あるいは自分で特徴量の抽出方法をカスタマイズすることもできます。

#### scenePrintのリビジョン番号について

記事の冒頭でも説明しましたが、iOS 17でリビジョン番号として`nil`を指定するとエラーが発生してモデルの訓練ができません。iOS 15では問題なく動作するため、おそらくドキュメントの文面は正しくて実装に不具合があるものと思われます。

リビジョン番号と利用可能なiOSのバージョンは次のとおりです。

- Revision 1: iOS 15以降で利用可能
- Revision 2: iOS 17以降で利用可能

iOSバージョンごとの挙動は次のとおりです。

- iOS 15: `1`または`nil`を指定できる。`2`を指定するとエラーが発生する。
- iOS 16: 手元に実機がないため未確認。
- iOS 17: `1`または`2`を指定できる。`nil`を指定するとエラーが発生するが、このエラーはおそらく不具合。

なお、サポートされていないリビジョン番号を指定すると以下のようなエラーメッセージが表示されます。

```text
（iOS 15でリビジョン番号として2を指定）

Revision 2 is not supported. Please try revision 1.
```

### 3. MLTrainingSessionParametersのデフォルト値に注意

MLImageClassifierのクラスメソッド`train()`についての話です。シグネチャは次の通りです。

```swift
static func train(
  trainingData: MLImageClassifier.DataSource,
  parameters: MLImageClassifier.ModelParameters = ModelParameters(
      validation: .split(strategy: .automatic),
      augmentation: [],
      algorithm: .transferLearning(
        featureExtractor: .scenePrint(revision: 1),
        classifier: .logisticRegressor
      )
    ),
  sessionParameters: MLTrainingSessionParameters = _defaultSessionParameters
) throws -> MLJob<MLImageClassifier>
```

ここで引数`sessionParameters`に注目してください。デフォルト値が設定されていますが、具体的にどのような値なのかApple Developerのドキュメントに説明がありません。

続いて`MLTrainingSessionParameters`の`init()`がどのように定義されているか確認してみましょう。

```swift
init(
  sessionDirectory: URL? = nil,
  reportInterval: Int = 5,
  checkpointInterval: Int = 10,
  iterations: Int = 1000
)
```

さて、`_defaultSessionParameters`とMLTrainingSessionParametersの`init()`が返す値は同じなのでしょうか？

答えはNOです。両者のデフォルト値は異なります。

どのようなアプリを実装するかに依存しますが、おそらく`init()`が返すデフォルト値は通常のユースケースでは頻度が細かすぎる上にイテレーションの回数が多すぎます。特に理由がなければ`train()`の`sessionParameters`は省略して問題ありません。

### 4. MLJobのcheckpointsプロパティで訓練の進捗を取得するにはMLImageClassifierの`train()`にsessionParameters引数を指定する必要がある

MLImageClassifierの`train()`で訓練を開始した場合の話です。`sessionParameters`は省略するのがおすすめと説明しましたが、そうすると訓練の進捗が取得できなくなります。訓練に相当な時間が必要になると見込まれる場合は`sessionParameters`を設定して、MLJobのcheckpointsで進捗を受け取れるようにしましょう。

なお`sessionParameters`でセッションを保存するように指示する場合、訓練途中に中断するケースの考慮が必要になります。具体的にはMLTrainingSessionParametersの`sessionDirectory`で指示したディレクトリにセッションが記録されたファイルが残ります。

この状態で再度MLImageClassifierの`train()`を実行するとエラーが発生します。途中からではなく訓練を最初から再実行したい場合はセッションが記録されたファイルを削除する必要があります。

### 5. MLJobのresultプロパティで訓練が完了した際の結果を受け取るにはAnyCancellableインスタンスを保持しておく必要がある

Create MLとは関係のない話題ですが、AnyPublisher型のプロパティから値を取得する場合の話です。PublisherはCombineフレームワークが提供するバインディング機構です。詳細については以下のドキュメントを参照してください。

- [Combine - Apple Developer Documentation](https://developer.apple.com/documentation/combine)
- [Publisher - Apple Developer Documentation](https://developer.apple.com/documentation/combine/publisher)
- [AnyPublisher - Apple Developer Documentation](https://developer.apple.com/documentation/Combine/AnyPublisher)
- [AnyCancellable - Apple Developer Documentation](https://developer.apple.com/documentation/combine/anycancellable)

なお、MLJobの訓練を中断したい場合はMLJobの`cancel()`メソッドを実行してください。

実装例

```swift
// 動作する
self.resultCancellable = self.job?.result.sink(
  receiveCompletion: { [weak self] completion in
    // ...
  },
  receiveValue: { [weak self] classifier in
    // ...
  }
)

// 動作しない
self.job?.result.sink(
  receiveCompletion: { [weak self] completion in
    // ...
  },
  receiveValue: { [weak self] classifier in
    // ...
  }
)
```

## よくある質問と回答
### 1. 訓練データの解像度に制限はある？

解像度に制限はありません。ただし、訓練データの画像は299 x 299の正方形に拡大または縮小された後に学習されます。MLModelのインスタンスを`print()`で表示すると学習に使用された入力データの形式が確認できます。

なお画像が長方形の場合、例えば縦方向に長い画像であれば、上下を切り取って正方形に加工してから学習されます。切り取りは画像の中心を軸に行われます。

従って、299 x 299より高解像度な画像を訓練データに使用しても、モデルの性能には影響しません。最終的に作成される`.mlmodel`ファイルのサイズが変化しないことからも確認できます。

ただし、画像の内容によっては拡大・縮小の前後でモアレやジャギーが発生して見た目が変化する恐れがあります。Create MLが内部的に使う画像リサイズのアルゴリズムは非公表のため、予期しない結果を防ぐには事前に画像の解像度を299 x 299に揃えてから訓練を開始することをおすすめします。

### 2. 訓練データに使用できる画像の形式は？

以下の形式が利用可能です。

- JPEG
- HEIF
- PNG
- TIFF

Core ImageフレームワークのCIContextで書き出せる形式であればすべて対応しています。

### 3. 訓練データに異なる解像度・異なる形式の画像を混ぜて問題ない？

問題ありません。ただし、例えば低画質なJPEG画像と高画質なPNG画像を混ぜて学習した場合、JPEG画像のノイズが特徴量として学習されるかもしれません。この場合であれば、PNG画像を事前にJPEG画像に変換して、訓練データの品質のばらつきを抑えるなどの工夫が必要になるかもしれません。

なお画像の解像度と形式は異なるものを混ぜて構いませんが、分類ラベルごとの画像の枚数は一定に保つ必要があります。例えばラベルAの訓練データは10枚、ラベルBの訓練データは1000枚のように、訓練データの個数にばらつきがあるとモデルの性能が低下する恐れがあります。

### 4. 訓練用の画像を作成中に「fopen failed for data file: errno = 2 (No such file or directory)」が表示される

Create MLとは関係ありませんが、遭遇する可能性が高いので説明します。訓練用の画像ファイルを大量に作成すると、以下のエラーが表示されるはずです。ネットからダウンロードしたりカメラで撮影したり、手段は関係ありません。

```text
（日付などのプレフィックスは省略しています）

fopen failed for data file: errno = 2 (No such file or directory)
Errors found! Invalidating cache...
```

エラーが発生しているように見えますが、エラーではありません。紛らわしいのですが、無視して問題ありません。

iOSはアプリごとにサンドボックス環境が作成されます。そのファイルシステムではキャッシュ管理にSQLLiteが使用されています。

キャッシュ管理はある程度ファイルの書き込みをしなければ発動しません。そのためアプリのインストール直後はSQLLiteファイルが存在せず、ある程度画像ファイルが作成された段階で上記のエラーメッセージが表示されます。

### 5. 何も設定していないのにモデルの訓練の進捗状況がコンソールに表示される

Create MLのお節介な機能のようです。iOS 15とXcode 14.2の組み合わせですと、以下のようなメッセージが出力されます。無効化はできません。

```text
...
Processing 10 files
Processing 10 files
Processing 10 files
Processing 8 files
Extracting image features from training data.
Extracting image features from user validation data.
Beginning model training on processed features. 
Calibrating solver; this may take some time.
+-----------+--------------+-------------------+---------------------+
| Iteration | Elapsed Time | Training Accuracy | Validation Accuracy |
+-----------+--------------+-------------------+---------------------+
| 0         | 1.113087     | 0.932418          | 0.938596            |
| 1         | 1.303130     | 0.961985          | 0.912281            |
| 2         | 1.600825     | 1.000000          | 1.000000            |
| 3         | 1.682178     | 1.000000          | 1.000000            |
| 4         | 1.768306     | 1.000000          | 1.000000            |
| 9         | 2.360121     | 1.000000          | 1.000000            |
| 10        | 2.443858     | 1.000000          | 1.000000            |
+-----------+--------------+-------------------+---------------------+
Trained model successfully saved at /var/mobile/Containers/Data/Application/87BA36EA-275F-4396-8361-8963FF4A67EE/Documents/MLModel/NewsModelV1.mlmodel.
```

iOS 17とXcode 15の組み合わせでは上記のような進捗状況のメッセージは表示されません。

## 参考資料

1. [MLJob - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mljob)
2. [MLImageClassifier.ModelParameters - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct)
3. [MLImageClassifier.FeatureExtractorType - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/featureextractortype)
4. [MLImageClassifier.ModelParameters.ModelAlgorithmType - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct/modelalgorithmtype)
5. [iOSでのCreateML / CoreMLを用いたオンデバイス機械学習 - エニグモ開発者ブログ](https://tech.enigmo.co.jp/entry/2022/12/20/070000)
