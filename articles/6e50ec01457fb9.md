---
title: "（iOS）Audio Unit v3 Extensionでシンセサイザを作る"
emoji: "📑"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---
## はじめに

Audio Unit v3 Extensionでシンセサイザを作るのは簡単です。Xcodeを開いてFile→New→ProjectからAudio Unit Extensionテンプレートを選ぶだけです。あとはテンプレートの実装を参考に各自で調査しながら実装してください。

そうですね、この説明だけで実装できるなら苦労はしません。おそらく、本記事にたどりついた方は何から着手すればよいか情報が不足している状況かと思います。

本記事ではノコギリ波の音色を合成するシンセサイザアプリを作成します。USB MIDIキーボードからの入力またはアプリ画面に配置されたボタンのタッチをトリガーにして音声を再生するアプリです。

### 動作環境

本記事で説明する内容は以下の環境で動作確認をしました。

- 開発環境: Xcode 14からXcode 16
- プラットフォーム: iOS 15からiOS 18

また、以下のiPhone実機で動作確認をしています。

- iPhone 15 Pro / iOS 18.2.1
- iPhone 13 / iOS 15.7.1)
- iPhone SE（第2世代） / iOS 18.2.1

Lightning端子を搭載した機種についてはApple純正品のCamera Connection Kitアダプタを利用してUSB MIDIキーボードを接続しました。

### 必要になるC++ / Objective-C / Swiftの知識について

本記事はXcodeが生成するテンプレートからなるべく逸脱しないように説明を進めます。テンプレートに加える変更は最小限を目指すため、Objective-Cのコードについては読み書きしません。

また、本記事で説明する範囲であれば、C++とSwiftどちらかの言語の読み書きができれば片方の言語は見よう見まねで実装できるかと思います。もちろん両方の言語に習熟していると理解は早まるかと思います。

なお、今回はObjective-Cコードの読み書きはしませんが、以下のドキュメントに目を通すことをお勧めします。

### VSTやAAXの知識

### （補足）Xcode 16のAudio Unit Extensionテンプレートについて

Xcode 16で新規プロジェクトを作成する場合、プラットフォームとしてiOSを選ぶとテンプレート一覧にAudio Unit Extensionが表示されません。Audio Unit Extensionはマルチプラットフォームを選択した場合のみ表示されます。

なお、マルチプラットフォームのAudio Unit ExtensionをiPhone実機で実行しようとすると「Failed to verify code signature」エラーが発生してインストールできない事象を確認しています。私個人のXcode設定に不備がありそうですが、現在も調査中です。もし同じ事象が発生する場合はコメントなどで報告いただけると助かります。

## サンプルアプリ

本記事で実装するシンセサイザアプリの完成版を以下のGitHubリポジトリで公開しています。対象のバージョンはXcode 14以上、iOS 15以上です。

https://github.com/moutend/SawtoothSynthesizer

## Audio Unit v3 Extensionの構造

それではシンセサイザアプリの開発をはじめましょう。まずはiOSにおけるAudio Unit v3 Extensionの構造について全体像を説明します。

### Audio Unit Extensionは独立したプロセスとして実行される

Audio Unit v3 Extensionは通常のアプリとは異なり、独立したプロセスとして実行されます。以降、Audio Unit v3 Extensionの実行を要求する側のアプリをホストアプリと呼びます。

Xcodeの新規プロジェクトでAudio Unit Extensionを選択すると、ホストアプリとAudio Unit Extensionのセットでプロジェクトが作成されます。audio Unit Extensionはホストアプリと独立しているため、既存のアプリにFile→New→TargetからAudio Unit Extensionを追加することも可能です。

### MIDIの入出力

ホストアプリとAudio Unit Extensionはプロセス間通信を利用してMIDIの入出力を行います。MIDI入力の流れは外部MIDI機器→ホストアプリ→Audio Unit Extensionです。出力の場合は流れが逆になります。

従って、Audio Unit Extensionが直接外部のMIDI機器と通信することはできません。入出力はホストアプリを経由して行うことになります。

また、信号の流れからご想像のとおり、ホストアプリがMIDIのイベントを生成しても構いません。楽器アプリが外部MIDI機器を接続しなくても、画面上のピアノロールをタッチすると音声が流れるのはこれが理由です。

### 音声信号の合成と再生

ホストアプリとAudio Unit Extensionは音声信号のバッファを共有します。ホストアプリは共有されたバッファにAVAudioEngineのAVAudioUnitを経由してアクセスします。

従って、Audio Unit Extensionが直接iPhoneのハードウェアを制御してスピーカーから音声を再生するといったことはできません。

Audio Unit Extensionは音声信号のバッファを読み書きするだけです。その音声信号を再生するのはホストアプリの役割です。
