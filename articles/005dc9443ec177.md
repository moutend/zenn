---
title: "視覚障害者を補助するAI画像認識アプリ「スイフトアイ」を支える技術"
emoji: "👓"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [iPhone, AI, Gemini]
published: false
---
## はじめに

スイフトアイは視覚障害者を補助するAI画像認識アプリです。カメラで撮影した画像や写真アプリなどから共有された画像をAIが解析して、文章で読み上げます。  
以下のアプリ紹介動画（3:30頃から）をご覧いただくと、動作の様子がご確認いただけます。本記事では、このアプリ開発の裏側を少し紹介します。  

https://youtu.be/VZM-WIXfsWU?si=VVBS0rnDUmOd-gE2

## 処理はサーバーで行う

まずは処理の方式についてです。本アプリの画像解析はGoogle Geminiを利用してサーバー上で行っています。  
オンデバイスで動作する超小型のモデルも増えつつありますが、image-to-textタスクをオンデバイスで実行するにはハードウェアの性能が足りません。必然的にサーバーで処理する方式となります。  

**追記 2025-05-14**

困難であると説明した直後にオンデバイスで画像認識を行うHeronというアプリが登場しました。TURING MOTORSが開発した2Bパラメータのモデルを搭載しているそうです。  
早速iPhone 15 Proで試したところ、数百msのオーダーで画像の説明文が生成されました。驚異的です。  
ただし、間違った回答や意味不明な文章が出力されることも多いため、依然としてサーバー上でモデルを動作させる方式を置き換えるものではありません。とはいえ、オンデバイスで実現可能な範囲が広がる可能性を感じますし、今後の流れに注目です。  

https://apps.apple.com/jp/app/heron/id6745646268

## モデルには癖がある

今やマルチモーダルなモデルはありふれています。image-to-textのような基本的タスクをこなすだけであれば、モデルの差は生じないように思えるかもしれません。  しかし、実際に試してみるとモデルごとに固有の癖があることに気がつきます。

例

- 詳細より全体像を優先する: 画像に含まれているテキストを書き起こしさせると、画像全体の様子から類推されるキーワードで内容を置き換えてしまう。
- 過剰に説明的すぎる: 画像の中に同じようなパターンが繰り返し現れる場合に似たような説明を何度も繰り返してしまう。
- 回答が曖昧すぎる: 間違った解答を避けようとして無難でつかみどころのない解答を返してしまう。

多くのモデルを試した結果、記事を投稿した時点ではGoogle Gemini 2.0 Flashが解答の速度と内容の正確さで最もバランスが取れていると感じています。ただし、あくまで現時点での評価であり、今後より高性能なモデルが登場する可能性もあります。  
なお、モデルの評価方法については秘伝のタレということで本記事では詳細を省きます。１点だけヒントを与えると、利用者の立場になって徹底的に考えるのがコツです。  
例えば、目が見えなくても触って判断できるものや様子について、あえて画像認識アプリで解析させる頻度は低いと予想できます。そうすれば、どのような画像でモデルを評価するべきか、対象は絞られていくはずです。闇雲にあらゆる状況で万能なモデルを探すより実用度は高まります。  

## プロンプトエンジニアリングは行わない

ここでは期待する回答へ誘導する行為をプロンプトエンジニアリングと定義します。エンジニアリングの程度は製品によって様々ですが、例えば開発者向けのコーディング支援ツールではその膨大なプロンプトが度々話題になります。  
本アプリにあてはめると、例えば視覚に障害のある方への説明方法として有名なのがクロックポジションです。例えば風景を撮影した画像があるとして、10時の方向に建物がある、といった具合にアナログ時計の文字盤でポジションを伝える方式です。  
それでは、スイフトアイ固有のプロンプトとして、どのような内容を指定しているのでしょうか？以下に全文を掲載します。  

> この画像には何が写っていますか？視覚に障害のあるユーザーを想定し、文章だけで内容が伝わるように回答をお願いします。

以上です。驚くべきことに、上記のプロンプトで記事冒頭に紹介した動画のような回答が得られます。  
つまり、プロンプトエンジニアリングを全く行っていません。Google Geminiのような汎用モデルは視覚障害から連想される要件も暗黙に汲み取って解答を生成してくれます。下手にあれこれ指示するよりも素朴な指示の方がモデルの性能を引き出せる場合もあるのです。  

## 画像の解像度にはスイートスポットがある

image-to-textタスクで問題になるのがモデルへ入力する画像の解像度です。大きければ画像の隅々まで詳細を正確に判断できますし、小さければ高速なレスポンスが期待できます。  
それでは、スイフトアイの場合はどのように処理しているかというと、解像度が1 MP以下になるようにリサイズしてからGeminiに入力しています。この1 MPという数値は計算で求めたのではなく、画像認識を何度も試しながら得られた訂正的な評価に基づきます。  
商品パッケージに書かれたテキストの文字起こしや風景写真の解析など、利用シーンは様々あります。しかし、どのシーンでも最低限確保するべき解像度はあまり大差ないことがわかりました。  
このようなパラメータのスイートスポットは実際に試さなければわかりません。さらに、この値はあくまで現時点でのニーズに基づいた値であり、広く一般に適用できるものでもありません。
生成AIはあくまでプロダクトを実現するビルディングブロックの一部に過ぎません。制御可能なパラメータの中で、快適なユーザー体験に影響する部分を見極めてチューニングするのは依然として人間の役割です。  

ちなみに、記事を投稿した時点ではアプリのユーザーが増加しはじめて、紙書籍をOCRする用途で利用されることも増えています。iOSであれば、Core MLに画像分類モデルがありオンデバイスで動作させることが可能です。入力画像が書類っぽいか判定して、該当する場合はOCRに特化したモデルへ、少し解像度を増やして画像を与える、というフローを検討しているところです。  

## おわりに

画像の内容をテキストで説明するだけの一見単純なアプリではありますが生成AIを活用する題材として思いのほか奥が深いです。需要があれば、もう少し深掘りした記事も書こうかと思います。  
