---
title: "（iOS 16）VoiceOverの読み上げ音声を実装する手順"
emoji: "✨"
type: "tech"
topics: [iOS, VoiceOver, AudioUnit, 音声合成]
published: false
---
# はじめに

iOS 16にて激アツな機能が搭載されたのをご存知でしょうか？そうです、VoiceOverの音声として独自の読み上げ音声をインストールできるようになったのです。

- （参考）[Creating a custom speech synthesizer - Apple Developer](https://developer.apple.com/documentation/avfaudio/audio_engine/audio_units/creating_a_custom_speech_synthesizer?language=objc)

この記事ではiOSむけの読み上げ音声を実装する手順を説明します。音声合成エンジンを一から実装するのはハードルが高すぎるため、今回は特定のキーワードが渡された場合にあらかじめ作成した音声ファイルを再生するアプリを作成します。

なお、記事のタイトルはiOS 16としましたが、独自の読み上げ音声はipadOS 16とmacOS 13 Ventura以降のバージョンで利用可能です。また、インストールした読み上げ音声はVoiceOverの他にアクセシビリティの読み上げコンテンツなどからも利用できるようになります。

## 環境

記事の投稿にあたり、以下の環境で動作確認を行いました。

- Xcode 14.3.1 (14E300c)
- macOS Ventura 13.4.1
- iOS 16.5.1

# プロジェクトの作成

読み上げ音声アプリは2つのアプリで構成されます。

1. 音声合成アプリ: AudioUnit Extensionとして実装されたアプリ。テキストを受け取り音声波形を合成する。通常はGUIを持たない。
2. コンテナアプリ: 音声合成アプリを埋め込んだアプリ。AudioUnit Extension単体をAppStoreで配布することは可能だが、楽器アプリとして認識されるのを防ぐためコンテナアプリに埋め込む形で配布する。

それではプロジェクトの作成手順について説明します。

## （Step 1）コンテナアプリの作成

1. Xcodeを起動し、メニューのFile→New→Projectを選びます。
2. プラットフォームはiOS、テンプレートはAppを選択します。
3. プロダクト名は何でも構いません。ここでは仮に「HelloSpeech」とします。
4. インターフェースはSwiftUI、言語はSwiftを選びます。「Use Core Data」と「Include Tests」のチェックは外してください。
5. プロジェクトが作成されたら完了です。

コンテナアプリはあくまでAudioUnit Extensionを配布するための入れ物です。生成されたContentView.swiftとHelloSpeechApp.swiftの修正は不要です。

## （Step 2）音声合成アプリの作成

1. Project NavigatorでHelloSpeechプロジェクトが選択された状態であることを確認します。
2. Add Targetボタンを押します。プラットフォームはiOS、テンプレートはAudio Unit Extensionを選びます。
3. プロダクト名は何でも構いません。ここでは仮に「Synth」とします。
4. Organization Nameを設定します。この値が音声一覧に表示されます。今回は「カスタム音声」とします。
5. Audio Unit Typeとして「Speech Synthesizer」を選びます。Subtype CodeとManufacturer Codeは何でも構いません。ここでは仮に「demo」と設定します。
6. ユーザーインターフェースを設定します。初期値として「Presents User Interface」が設定されている場合は「No User Interface」に変更します。
7. Project / Embed in Applicationを設定します。どちらも「HelloSpeech」を選びます。
8. Finishボタンを押して完了です。このとき「Activate “Synth” scheme?」というダイアログが表示された場合はActivateボタンを押します。

## （Step 3）コンテナアプリの埋め込み設定

1. ターゲット一覧にHelloSpeechとSynthが表示されていることを確認します。その後、HelloSpeechターゲットを選びます。
2. General / Signing & Capabilities / Resource ...と並んでいるメニューの中からGeneralを選択します。
3. Frameworks, Libraries, and Embedded Contentを開きます。
4. Linked Binaries一覧の中にSynth.appexが表示されていることを確認します。
5. 表示されていない場合はAddボタンを押してSynth.appexを追加します。

## （Step 4）コードの修正

以下2つのファイルを修正します。

1. Synth/Common/AudioUnitFactory/AudioUnitFactory.swift
2. Synth/Common/Audio Unit/SynthAudioUnit.swift

元のコードは消して、それぞれ次の内容に書き換えてください。

### AudioUnitFactory.swift

```swift
import CoreAudioKit

public class AudioUnitFactory: NSObject, AUAudioUnitFactory {
  var audioUnit: AUAudioUnit?

  public func beginRequest(with context: NSExtensionContext) {
  }
  @objc
  public func createAudioUnit(with componentDescription: AudioComponentDescription) throws
    -> AUAudioUnit
  {
    self.audioUnit = try SynthAudioUnit(componentDescription: componentDescription, options: [])

    return self.audioUnit!
  }
}
```

### SynthAudioUnit.swift

```swift
import AVFoundation

public class SynthAudioUnit: AVSpeechSynthesisProviderAudioUnit {
  private var request: AVSpeechSynthesisProviderRequest?
  private var outputBus: AUAudioUnitBus
  private var _outputBusses: AUAudioUnitBusArray!
  private var currentBuffer: AVAudioPCMBuffer?
  private var framePosition: AVAudioFramePosition = 0
  private var format: AVAudioFormat

  @objc
  override init(
    componentDescription: AudioComponentDescription,
    options: AudioComponentInstantiationOptions
  ) throws {
    let basicDescription = AudioStreamBasicDescription(
      mSampleRate: 22050,
      mFormatID: kAudioFormatLinearPCM,
      mFormatFlags: kAudioFormatFlagsNativeFloatPacked | kAudioFormatFlagIsNonInterleaved,
      mBytesPerPacket: 4,
      mFramesPerPacket: 1,
      mBytesPerFrame: 4,
      mChannelsPerFrame: 1,
      mBitsPerChannel: 32,
      mReserved: 0)

    self.format = AVAudioFormat(
      cmAudioFormatDescription: try! CMAudioFormatDescription(
        audioStreamBasicDescription: basicDescription))
    self.outputBus = try AUAudioUnitBus(format: self.format)

    try super.init(
      componentDescription: componentDescription,
      options: options)
    self._outputBusses = AUAudioUnitBusArray(
      audioUnit: self,
      busType: AUAudioUnitBusType.output,
      busses: [outputBus])
  }

  public override var speechVoices: [AVSpeechSynthesisProviderVoice] {
    get {
      return [
        AVSpeechSynthesisProviderVoice(
          name: "声1", identifier: "com.HelloSpeech.Synth.Voice1", primaryLanguages: ["ja-JP"],
          supportedLanguages: ["ja-JP"])
      ]
    }
    set {}
  }

  public override var outputBusses: AUAudioUnitBusArray {
    return self._outputBusses
  }
  public override func allocateRenderResources() throws {
    try super.allocateRenderResources()
  }
  public override var internalRenderBlock: AUInternalRenderBlock {
    return { actionFlags, timestamp, frameCount, outputBusNumber, outputAudioBufferList, _, _ in
      let unsafeBuffer = UnsafeMutableAudioBufferListPointer(outputAudioBufferList)[0]
      let frames = unsafeBuffer.mData!.assumingMemoryBound(to: Float32.self)
      let sourceBuffer = UnsafeMutableAudioBufferListPointer(
        self.currentBuffer!.mutableAudioBufferList)[0]
      let sourceFrames = sourceBuffer.mData!.assumingMemoryBound(to: Float32.self)

      for frame in 0..<frameCount {
        frames[Int(frame)] = 0.0
      }
      for frame in 0..<frameCount {
        frames[Int(frame)] = sourceFrames[Int(self.framePosition)]
        self.framePosition += 1

        if self.framePosition >= self.currentBuffer!.frameLength {
          actionFlags.pointee = .offlineUnitRenderAction_Complete
          break
        }
      }

      return noErr
    }
  }

  public override func synthesizeSpeechRequest(_ speechRequest: AVSpeechSynthesisProviderRequest) {
    self.request = speechRequest
    self.currentBuffer = getAudioBufferForSSML(speechRequest.ssmlRepresentation)
    self.framePosition = 0
  }
  public override func cancelSpeechRequest() {
    self.request = nil
  }
  func getAudioBufferForSSML(_ ssml: String) -> AVAudioPCMBuffer? {
    let audioFileName = ssml.contains("Hello") ? "Hello" : "Goodbye"

    guard
      let fileUrl = Bundle.main.url(
        forResource: audioFileName,
        withExtension: "aiff")
    else {
      return nil
    }
    do {
      let file = try AVAudioFile(forReading: fileUrl)
      let buffer = AVAudioPCMBuffer(
        pcmFormat: self.format,
        frameCapacity: AVAudioFrameCount(file.length))
      try file.read(into: buffer!)

      return buffer
    } catch {
      return nil
    }
  }
}
```

## （Step 5）音声ファイルの取り込み

最後のステップです。プロジェクトへ音声ファイルを追加します。

ターミナルを開き、以下のコマンドを実行します。成功するとHello.aiffとGoodbye.aiffファイルが作成されます。

```console
$ say -o Hello.aiff 'Hello'
$ say -o Goodbye.aiff 'Goodbye'
```

Xcodeに戻り、音声ファイルを追加します。手順は次のとおりです。

1. Xcodeを開きます。Project NavigatorでSynthグループを選びます。
2. Synthグループの上で右クリックしてメニューを開き、New Groupを選びます。名前は「Audio」を入力してください。
3. Audioグループの上で右クリックしてメニューを開き、Add Files to “HelloSpeech“ …を選びます。
4. ファイル選択のダイアログが表示されます。先ほど作成したHello.aiffとGoodbye.aiffを選びます。
5. ファイル追加先の一覧が表示されます。HelloSpeechのチェックは外し、Synthにチェックをつけます。
6. Copy items if neededにチェックをつけます。
7. フォルダの設定は「Create folder references」を選びます。
8. Addボタンを押して完了です。

# 試運転

準備は整いました。実機を使って動作確認をしましょう。手順は次のとおりです。

1. Xcodeのメニューを開き、Product→Scheme→Manage Schemesを選びます。
2. Autocreate Schemes Nowボタンを押します。完了したらCloseボタンを押してダイアログを閉じてください。
3. Xcodeのメニューを開き、Product→Schemeを選びます。
4. HelloSpeechを選びます。
5. `Command + R`キーを押してプロジェクトを実行します。
6. 実機でアプリが起動したら、設定アプリを開き、アクセシビリティを選びます。
7. VoiceOver→読み上げ→声と進み、カスタムボイスを選びます。
8. 「声1」を選択するとカスタム音声で読み上げが開始されます。今回は「ハロー」と「グッバイ」の音声しか用意していないため、項目にフォーカスが当たると「グッバイ、グッバイ、グッバイ...」と連呼します。

以上がカスタム読み上げ音声の実装手順です。お手軽に実装できることが理解いただけたかと思います。

# よくある質問

カスタム読み上げ音声を実装する上で生じる疑問についてまとめます。

なお、以下で説明する挙動はiOS 16.5.1の実機で確認したものです。今後のバージョンで挙動が変更されるかもしれませんのでご留意ください。

## （Q. 1）synthesizeSpeechRequest()メソッドが実装されていない場合どうなる？

システム組み込みのデフォルト音声にフォールバックされて読み上げされます。例えば日本語の場合はKyoko、英語（US English）の場合はSamanthaの声で読み上げされます。

## （Q. 2）AVSpeechSynthesizerのspeakメソッドあるいはwriteメソッドを呼び出すとどうなる？

エラーを吐かずサイレントに失敗します。アプリがクラッシュすることはなく、ログにも記録されません。つまり、メソッドを呼び出していないかのような振る舞いをします。

例えば以下のようなコードを実装しても音声は読み上げされません。

```swift
public override func synthesizeSpeechRequest(_ speechRequest: AVSpeechSynthesisProviderRequest) {
  let synthesizer = AVSpeechSynthesizer()
  let voice = AVSpeechSynthesisVoice.init(language: "ja-JP")
  let utterance = AVSpeechUtterance.init(string: speechRequest.ssmlRepresentation)

  utterance.voice = voice
  synthesizer.speak(utterance)
}
```

仮にAVSpeechSynthesizerのspeakやwriteメソッドを許可すると読み上げ音声として自分自身を設定した場合に処理が循環してしまいます。そのような動作を防ぐための挙動と思われます。

ただ、iOS 16からAVSpeechSynthesizerの挙動がおかしい、というバグ報告もあります。システム組み込みの音声を利用するのであれば処理の循環は発生しませんし、もしかしたら仕様ではなくバグということでiOS 17以降で修正されるかもしれません。

- （参考）[AVSpeechSynthesizer not working in… | Apple Developer Forums](https://developer.apple.com/forums/thread/714984)

## （Q. 3）AudioUnit Extension内のprint文が出力されない

AudioUnit Extensionはアウトプロセスで実行されます。従って、print文の内容はXcodeのデバッグコンソールには表示されません。

この問題の回避策としてはUnified loggingを利用するのが簡単です。

```swift
// ynth/Common/Audio Unit/SynthAudioUnit.swift
import AVFoundation
import os

private let logger = Logger(subsystem: Bundle.main.bundleIdentifier!, category: "SynthAudioUnit")

public class SynthAudioUnit: AVSpeechSynthesisProviderAudioUnit {
  // ...
}
```

ログは以下のように出力します。

```swift
public override func synthesizeSpeechRequest(_ speechRequest: AVSpeechSynthesisProviderRequest) {
  logger.log("Synth: ssmlRepresentation: \(speechRequest.ssmlRepresentation, privacy: .public)")
  // ...
}
```

ログの取得にはmacOS組み込みの`log`コマンドを利用します。Xcodeを開いて`Command + R`でアプリを起動し、その後に以下のコマンドを実行します。

```swift
$ sudo log collect \
  --device-name 'XcodeのProduct→Destinationに表示されるデバイス名' \
  --last 10m
```

`--last 10m`は直近10分間のログを収集することを意味します。

コマンドの実行が完了すると`system_logs.logarchive`という名前のディレクトリが作成されます。ログの量にもよりますが、コマンドが完了するまで数十秒から数分かかります。

収集されたログは`log show system_logs.logarchive`コマンドを実行することでテキストとして閲覧できます。あるいは`open system_logs01.logarchive`コマンドを実行するとConsole.appが起動して読みやすい形で表示されます。

Unified loggingの詳細については以下の資料を参照してください。

## （Q. 4）リクエストとして渡されるSSMLの仕様を詳しく知りたい

私も知りたいです。残念ながら、AVSpeechSynthesisProviderRequestのドキュメントには「ピッチや速度、イントネーションなどの情報が含まれるよ」といった曖昧な説明しか書かれていません。

- （参考）[AVSpeechSynthesisProviderRequest - Apple Developer](https://developer.apple.com/documentation/avfaudio/avspeechsynthesisproviderrequest)

ここからは実機で動作検証をして得られた知見についてまとめます。ドキュメントには記載されていないため、仕様が突然変更される可能性があることを承知の上で読み進めてください。

### 基本構造

ssmlRepresentationに格納される文字列について、基本は以下の構造でマークアップされています。読みやすさのためにインデントと改行を加えています。

```xml
<speak>
  <prosody rate="400.0%" volume="-1.0dB">
    読み上げ対象の文字列
  </prosody>
</speak>
```

現時点で判明している仕様をまとめます。

1. XMLのスキームを表す文字列（`<?xml version=...>`）は含まれない。
2. ルート要素は常に`<speak>`要素である。
3. `<speak>`要素について、`<speak version="1.1">`のような属性が設定されることはない。
4. `<speak>`要素の子要素として常に`<prosody>`要素が含まれる。
5. `<prosody>`の子要素として読み上げ対象の文字列が格納される。
6. `<prosody>`要素には常に`rate`と`volume`属性が含まれる。
7. 読み上げの速度を示す`rate`属性の値とVoiceOverローターの読み上げ速度は一致するとは限らない。例えば読み上げの言語が日本語の場合、読み上げ速度を100%に設定すると`<prosody>`の`rate`属性には値として`400.0%`が設定される。
8. 読み上げの音量を示す`volume`属性の値とVoiceOverローターの読み上げ速度は一致するとは限らない。例えば読み上げの言語が日本語の場合、読み上げ音量を100%に設定すると`<prosody>`の`volume`属性には値として`-1.0dB`が設定される。

### SSMLのサンプル

実機で取得したSSMLのサンプルをいくつか紹介します。

#### 基本形

```xml
<speak>
  <prosody rate="400.0%" volume="-1.0dB">
    カスタムのアクションを選択するには、上または下にスワイプします。その後ダブルタップしてアクティベートします。
  </prosody>
</speak>
```

特にひねりのないマークアップです。`<speak>`要素の中に`<prosody>`要素があり、その中に読み上げ対象の文字列が格納されています。

#### 読み上げの待機

「設定...開くにはダブルタップします」のように、読み上げ項目の間に少しだけ間を開けることがあります。その際は以下のようなリクエストが届きます。

1つ目のリクエスト

```xml
<speak>
  <prosody rate="400.0%" volume="-1.0dB">
    設定
  </prosody>
</speak>
```

2つ目のリクエスト

```xml
<speak>
  <prosody rate="400.0%" volume="-1.0dB">
    <break time="400ms" />
  </prosody>
</speak>
```

3つ目のリクエスト

```xml
<speak>
  <prosody rate="400.0%" volume="-1.0dB">
    開くにはダブルタップします
  </prosody>
</speak>
```

`<break />`要素が登場しました。ここで注目したいのは、「`設定<break time="400ms" />開くにはダブルタップします`」のように1リクエストに読み上げ項目をすべて含めるのではなく、「`設定`・`400 msの待機`・`開くにはダブルタップします`」という3つのリクエストに分割されているところです。
#### インライン要素

設定→アクセシビリティ→VoiceOver→読み上げ→声→Otoyaと進み、Otoyaを選択するためのボタンにフォーカスが当たると以下のリクエストが届きます。

```xml
<speak>
  <prosody rate="400.0%" volume="-1.0dB">
    Otoya 拡張 ,<break time="178ms" /> 101 ドット 5 MB使用
  </prosody>
</speak>
```

読み上げ文字列の中に`<break />`要素が埋め込まれています。待機時間は178msが指定されており、1ms単位で待機時間を指定するパターンがあることがわかります。

あくまで推測ですが、400msのように長めの待機時間が設定される場合は前後の読み上げ対象の文字列でリクエストが分割されるのかもしれません。今のところ、読み上げ対象の文字列の中に`<break />`要素が埋め込まれる場合、その待機時間は200ms未満であるパターンしか見つけられていません。

#### 話者の指定

設定→アクセシビリティ→VoiceOver→読み上げと進み、声を選択するボタンにフォーカスが移動した際に以下のリクエストが届きます。

```xml
<speak>
  <prosody rate="400.0%" volume="-1.0dB">
    <say-as interpret-as="characters">声</say-as>
  </prosody>
</speak>
```

`<say-as>`要素が登場するのは設定アプリの中、それもVoiceOverの声を変更するためのボタンにのみマークアップされています。

今のところ、他のUIで`<say-as>`要素が登場する場面は見つかっていません。そもそも、どのようにUIを実装すればSSMLで`<say-as>`と認識されるのか不明です。

# 参考資料

1. [Creating a custom speech synthesizer - Apple Developer](https://developer.apple.com/documentation/avfaudio/audio_engine/audio_units/creating_a_custom_speech_synthesizer?language=objc)
2. [Extend Speech Synthesis with personal and custom voices - WWDC23 - Videos - Apple Developer](https://developer.apple.com/videos/play/wwdc2023/10033/)
3. [Logging - Apple Developer](https://developer.apple.com/documentation/os/logging#2878594)
4. [Generating Log Messages from Your Code](https://developer.apple.com/documentation/os/logging/generating_log_messages_from_your_code)
5. [ Viewing Log Messages](https://developer.apple.com/documentation/os/logging/viewing_log_messages)
6. [Customizing Logging Behavior While Debugging](https://developer.apple.com/documentation/os/logging/customizing_logging_behavior_while_debugging)
7. [Logger - Apple Developer](https://developer.apple.com/documentation/os/logger)
8. [AVSpeechSynthesisProviderAudioUnit - Apple Developer ](https://developer.apple.com/documentation/avfaudio/avspeechsynthesisprovideraudiounit)
