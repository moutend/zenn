---
title: "（Swift）iPhoneでCreate MLのMLImageClassifierを用いて画像分類モデルの訓練と推論を行う"
emoji: "📝"
type: "tech"
topics: [Apple, iOS, Swift, CreateML, 機械学習]
published: false
---
## はじめに

iPhoneでCreate MLのMLImageClassifierを用いて画像分類モデルの訓練と推論行うサンプルアプリを作成しました。iOS 15以上のデバイスで動作します。

https://github.com/moutend/TrainingMLImageClassifier

訓練済みモデルの`.mlmodel`ファイルをアプリに埋め込む例はよく見かけますが、モデルの訓練から推論までをiPhone上で完結させる例はあまり見かけません。オンデバイスで処理する一例として参考になれば幸いです。

## 不具合と思われる事象の報告

さて本題に進みたいところですが、不具合と思われる事象に遭遇したので共有します。実装の要点だけ知りたい場合、読み飛ばしていただいて構いません。

### 1. 訓練が極端に遅くなる事象について

原因は不明ですが、iOS 17でモデルの訓練にかかる時間が極端に遅くなる事象を確認しています。具体的にはiOS 15と比較して3倍ほど遅くなります。サンプルアプリを例にすると以下のような結果になりました。

- iOS 15とiPhone 13の組み合わせ→9.8秒
- iOS 17とiPhone 15 Proの組み合わせ→29.9秒

Appleのドキュメントによると、iPhone 13に搭載されているA15 Bionicチップの性能は15.8 TFLOPSです。iPhone 15 Proに搭載されているA17 ProチップのTFLOPS性能は公表されていませんが、A15世代と比較して2倍の性能だそうです。

従って、ハードウェアではなくiOSに原因がありそうです。Create MLフレームワークの内部的なパラメータが変更されて訓練が遅くなっているのかもしれません。なお推論にかかる時間はiPhone 15世代の方が高速です。

残念ながら解決策は見つけられていませんが、ひとまず事象の共有でした。もし何かご存知の方がいましたら、コメントなどで教えていただけたら幸いです。

### 2. 訓練中のエラー「Must allow 2048-element vector as output」について

iOS 17が搭載されたデバイスでモデルを訓練すると、XcodeのDebug Consoleに以下のエラーが表示されて処理が中断する場合があります。

```text
MLModelAsset: load failed with error Error Domain=com.apple.CoreML Code=0 "Must allow 2048-element vector as output" UserInfo={NSLocalizedDescription=Must allow 2048-element vector as output}
```

おそらくAppleの実装ミスがエラーの原因です。このエラーは`MLImageClassifier.FeatureExtractorType.scenePrint(revision: nil)`のように、リビジョン番号に`nil`を指定すると発生します。

Apple Developerのドキュメントによると、リビジョン番号として`nil`が指定された場合は最新の番号が指定されたものとして扱うと説明されています。

> The sceneprint version. The supported versions include 1 and 2. If nil defaults to the latest version.
> 
> 引用元 https://developer.apple.com/documentation/createml/mlimageclassifier/featureextractortype/sceneprint(revision:)

しかし、iOS 17では`nil`を指定すると上記のエラーが発生します。このエラーを回避するにはリビジョン番号として`1`か`2`を指定してください。

なお、このエラーはiOS 15では発生しません。詳細については実装の要点で改めて説明します。

## この記事について

Create MLのMLImageClassifierの利用方法についてはサンプルアプリをご確認ください。`Logic/ImageTrainer.swift`が訓練、`Logic/ImageClassifier.swift`が推論の実装になります。

この記事では実装の要点や開発中に生じる疑問についてお答えします。そもそもCreate MLが専門知識なしで手軽に扱えるフレームワークですので、機械学習についての前提知識は不要です。

ただし、機械学習の入門を済ませておくと記事を早く読み進められるかと思います。例えば書籍「ゼロから作るDeep Learning」がおすすめです。隅から隅まで読み込む必要はなく、概要を把握するだけでも十分です。

- [O'Reilly Japan - ゼロから作るDeep Learning](https://www.oreilly.co.jp/books/9784873117584/)

## 訓練から推論までの流れ

それではCreate MLを利用して画像分類器を作成・利用する流れを説明します。手順は以下の3ステップです。

1. 訓練データを用意します。具体的にはFileManagerで任意の場所にディレクトリを作成し、その中に画像ファイルを保存します。ディレクトリの名前が分類ラベルの名前になります。
2. 諸々のパラメータを設定して訓練を開始します。訓練が完了すると`.mlmodel`ファイルが作成されます。
3. 作成した`.mlmodel`ファイルをコンパイルします。その後、コンパイルされたモデルを読み込むと推論が可能な状態になります。

Create MLのお手軽さが伝わったでしょうか。

とはいえ、この時点で疑問が生まれます。

- iPhoneはストレージやメモリなどリソースが限られている。訓練データ（画像ファイル）のサイズや分類ラベルの個数に制限はあるのか？
- 訓練にはどれくらい時間がかかるのか？訓練中のリソース消費はどれくらいか？
- MacとiPhoneで訓練の内容は異なるのか？同じパラメータを指定して訓練した場合、最終的に作成される`.mlmodel`ファイルに違いはあるのか？
- モデルのコンパイルはどれくらい時間がかかるのか？コンパイルされたモデルの容量はどれくらいか？
- 推論にはどれくらい時間がかかるのか？推論のたびに都度コンパイルが必要なのか？コンパイルされたモデルを別のデバイスで再利用できるのか？

上記の疑問については記事の後半でお答えします。

## 実装の要点

ここからは実装の要点を説明します。項目は以下の5つです。

1. 実質的にロジスティック回帰モデルしか選択肢がない
2. ModelParametersのデフォルト値に注意
3. MLTrainingSessionParametersのデフォルト値に注意
4. MLJobのcheckpointsプロパティで訓練の進捗を取得するにはMLImageClassifierの`train()`にsessionParameters引数を指定する必要がある。
5. MLJobのresultプロパティで訓練が完了した際の結果を受け取るにはAnyCancellableインスタンスを保持しておく必要がある。

### 1. 実質的にロジスティック回帰モデルしか選択肢がない

画像分類に利用できるモデルは実質的にロジスティック回帰モデルのみです。Apple Developerドキュメントから引用します。

> `case logisticRegressor` -- Logistic regression is a statistical model that classifies input feature vector into different categories.
> `case multilayerPerceptron(layerSizes: [Int])` -- Multilayer perceptron, layerSizes holds a list of positive integers that represent the number of hidden units in each layer. An additional fully connected layer with a Softmax activation output will be added that maps to probabilities of sound categories.
> 
> 引用元 https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct/classifiertype

ロジスティック回帰モデルは機械学習モデルの一種です。流行りのニューラルネットワーク登場以前からある、技術的に枯れた手法です。

現状のCreate MLは2つの選択肢があり、1番目の`logisticRegressor`がロジスティック回帰モデルです。2番目の`multilayerPerceptron`がニューラルネットワークを利用したモデルです。しかし、2番目の説明を最後まで読むと音声を分類するため、と書かれています。

つまり、2番目の選択肢は画像処理でよく使われる畳み込みニューラルネットワークのようなモデルではなく、文字通りレイヤーを重ねただけの素朴なニューラルネットワークです。何にでも使えるので、とりあえず選択肢として含まれているようです。

余談になりますが、書籍「ゼロから作るDeep Learning」では素朴な2層のニューラルネットワークを用いてMNIST画像の分類気を実装する例が登場します。それなりに機能することが確認できておもしろい実験です。

### 2. ModelParametersのデフォルト値に注意

MLImageClassifier.ModelParametersについての話です。まずは`init()`の定義を確認しましょう。

```swift
init(
  validation: MLImageClassifier.ModelParameters.ValidationData = __Defaults.validation,
  maxIterations: Int = __Defaults.maximumIterations,
  augmentation: MLImageClassifier.ImageAugmentationOptions,
  algorithm: MLImageClassifier.ModelParameters.ModelAlgorithmType = __Defaults.algorithm
)
```

引数のデフォルト値は次のとおりです。

- validation: `.split(strategy: .automatic)`
- maxIterations: `25`
- augmentation: デフォルト値なし
- algorithm: `.transferLearning(featureExtractor: .scenePrint(revision: 1), classifier: .logisticRegressor)`

それぞれの引数の役割は次のとおりです。

- validation: 機械学習では汎化能力を獲得させるため、訓練データとテストデータを分割する必要があります。validationのデフォルト値は訓練データからテストデータを自動的に良い塩梅で分割することを指示します。通常はデフォルト値のままで問題ありません。
- maxIterations: 訓練のイテレーションを繰り返す回数の最大値を意味します。25という数値には特に根拠がなく、おそらく仮の値として置かれているだけです。この値は各自で調整する必要があります。
- augmentation: 訓練データの水増しを行うか指示します。画像のぼかしや傾きを加えることを指示できますが、どれくらいボケを加えるのかや傾斜の角度など細かな指示はできません。
- algorithm: モデルの訓練に使用するアルゴリズムを指定します。enumで選択肢が定義されていますが、今のところ利用できるのはこの選択肢のみです。あるいは自分で特徴量の抽出方法をカスタマイズすることもできます。

scenePrintのrevision番号のデフォルト値は`1`になっていますが、現在は`2`が利用可能です。なおバージョン番号ではなく`nil`を指定すると常に最新のバージョンを利用することを指示できます。

以上を踏まえた上でパラメータを作成すると、例えば次のようになります。

```swift
let parameters = MLImageClassifier.ModelParameters(
  validation: .split(strategy: .automatic),
  maxIterations: 50,
  augmentation: [],
  algorithm: .transferLearning(
    featureExtractor: .scenePrint(revision: nil),
    classifier: .logisticRegressor
  )
)
```

### 3. MLTrainingSessionParametersのデフォルト値に注意

MLImageClassifierのクラスメソッド`train()`についての話です。シグネチャは次の通りです。

```swift
static func train(
  trainingData: MLImageClassifier.DataSource,
  parameters: MLImageClassifier.ModelParameters = ModelParameters(
      validation: .split(strategy: .automatic),
      augmentation: [],
      algorithm: .transferLearning(
        featureExtractor: .scenePrint(revision: 1),
        classifier: .logisticRegressor
      )
    ),
  sessionParameters: MLTrainingSessionParameters = _defaultSessionParameters
) throws -> MLJob<MLImageClassifier>
```

ここで引数`sessionParameters`に注目してください。デフォルト値が設定されていますが、具体的にどのような値なのかApple Developerのドキュメントに説明がありません。

続いて`MLTrainingSessionParameters`の`init()`がどのように定義されているか確認してみましょう。

```swift
init(
  sessionDirectory: URL? = nil,
  reportInterval: Int = 5,
  checkpointInterval: Int = 10,
  iterations: Int = 1000
)
```

さて、`_defaultSessionParameters`とMLTrainingSessionParametersの`init()`が返す値は同じなのでしょうか？

答えはNOです。両者のデフォルト値は異なります。

どのようなアプリを実装するかに依存しますが、おそらく`init()`が返すデフォルト値は通常のユースケースでは頻度が細かすぎる上にイテレーションの回数が多すぎます。特に理由がなければ`train()`の`sessionParameters`は省略して問題ありません。

### 4. MLJobのcheckpointsプロパティで訓練の進捗を取得するにはMLImageClassifierの`train()`にsessionParameters引数を指定する必要がある

MLImageClassifierの`train()`で訓練を開始した場合の話です。`sessionParameters`は省略するのがおすすめと説明しましたが、そうすると訓練の進捗が取得できなくなります。訓練に相当な時間が必要になると見込まれる場合は`sessionParameters`を設定して、MLJobのcheckpointsで進捗を受け取れるようにしましょう。

なお`sessionParameters`でセッションを保存するように指示する場合、訓練途中に中断するケースの考慮が必要になります。具体的にはMLTrainingSessionParametersの`sessionDirectory`で指示したディレクトリにセッションが記録されたファイルが残ります。

この状態で再度MLImageClassifierの`train()`を実行するとエラーが発生します。途中からではなく訓練を最初から再実行したい場合はセッションが記録されたファイルを削除する必要があります。

### 5. MLJobのresultプロパティで訓練が完了した際の結果を受け取るにはAnyCancellableインスタンスを保持しておく必要がある

Create MLとは関係のない話題ですが、AnyPublisher型のプロパティから値を取得する場合の話です。PublisherはCombineフレームワークが提供するバインディング機構です。詳細については以下のドキュメントを参照してください。

- [Combine - Apple Developer Documentation](https://developer.apple.com/documentation/combine)
- [Publisher - Apple Developer Documentation](https://developer.apple.com/documentation/combine/publisher)
- [AnyPublisher - Apple Developer Documentation](https://developer.apple.com/documentation/Combine/AnyPublisher)
- [AnyCancellable - Apple Developer Documentation](https://developer.apple.com/documentation/combine/anycancellable)

なお、MLJobの訓練を中断したい場合はMLJobの`cancel()`メソッドを実行してください。

実装例

```swift
// 動作する
self.resultCancellable = self.job?.result.sink(
  receiveCompletion: { [weak self] completion in
    // ...
  },
  receiveValue: { [weak self] classifier in
    // ...
  }
)

// 動作しない
self.job?.result.sink(
  receiveCompletion: { [weak self] completion in
    // ...
  },
  receiveValue: { [weak self] classifier in
    // ...
  }
)
```

## よくある質問と回答

## 参考資料

1. [MLJob - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mljob)
2. [MLImageClassifier.ModelParameters - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct)
3. [MLImageClassifier.FeatureExtractorType - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/featureextractortype)
4. [MLImageClassifier.ModelParameters.ModelAlgorithmType - Apple Developer Documentation](https://developer.apple.com/documentation/createml/mlimageclassifier/modelparameters-swift.struct/modelalgorithmtype)
5. [iOSでのCreateML / CoreMLを用いたオンデバイス機械学習 - エニグモ開発者ブログ](https://tech.enigmo.co.jp/entry/2022/12/20/070000)
