---
title: "（Swift）機械学習を利用して音声ファイルを分類する"
emoji: "🗂"
type: "tech"
topics: [Apple, Swift, "Core ML", 機械学習]
published: true
---
## はじめに

この記事ではAppleの機械学習フレームワークCore MLを利用して音声ファイルを識別する手順を解説します。例えば人の声が記録されていたら`speech`、音楽が記録されていたら`music`に分類します。

といっても、難しい要素はありません。Appleは訓練済みの機械学習モデルを提供しているので、それを利用します。謎めいた数式が登場することは一切ありませんので、ご安心ください。

なお、記事の投稿にあたり、以下の環境で動作確認しました。

- macOS Monterey 12.6.7
- Apple Swift 5.7

## 実装

以下のコードを`main.swift`として保存してください。

```swift
import SoundAnalysis

class ResultsObserver: NSObject, SNResultsObserving {
  func request(_ request: SNRequest, didProduce result: SNResult) {
    guard let result = result as? SNClassificationResult else {
      return
    }
    guard let classification = result.classifications.first else {
      return
    }

    let timeInSeconds = result.timeRange.start.seconds
    let formattedTime = String(format: "%.2f", timeInSeconds)

    print("Time: \(formattedTime)")

    let percent = classification.confidence * 100.0
    let percentString = String(format: "%.1f%%", percent)

    print("\(classification.identifier): \(percentString) confidence.\n")
  }
  func request(_ request: SNRequest, didFailWithError error: Error) {
    print("The analysis failed: \(error.localizedDescription)")
  }
  func requestDidComplete(_ request: SNRequest) {
    print("The request completed successfully!")
  }
}

func run() {
  if CommandLine.arguments.count < 2 {
    return
  }

  let audioFileURL = URL(fileURLWithPath: CommandLine.arguments[1])
  let resultsObserver = ResultsObserver()

  do {
    let classifySoundRequest = try SNClassifySoundRequest(classifierIdentifier: .version1)

    classifySoundRequest.windowDuration = CMTimeMake(value: 10, timescale: 10)

    let audioFileAnalyzer = try SNAudioFileAnalyzer(url: audioFileURL)

    try audioFileAnalyzer.add(classifySoundRequest, withObserver: resultsObserver)

    audioFileAnalyzer.analyze()
  } catch let error {
    print(error)
  }
}

run()
```

### 試運転

ターミナルを開いて以下のコマンドを実行してください。

```console
$ swift <音声ファイルのパス>
```

上記のコマンドを実行すると、一定の時間（今回は0.5秒）ごとに分類を実行した結果が表示されます。

### （実行例その1）話し声の識別

macOSには音声合成コマンド`say`が搭載されています。生成した音声ファイルがどのように識別されるか試してみましょう。

まずは以下のコマンドを実行してください。

```console
$ say -v Kyoko -o Kyoko.aiff 'こんにちは、私の名前はKyokoです。日本語の音声をお届けします。'
```

以下のような結果が得られます。

```console
$ swift main.swift Kyoko.aiff
Time: 0.00
speech: 92.4% confidence.

Time: 0.50
speech: 93.2% confidence.

Time: 1.00
speech: 93.3% confidence.

Time: 1.50
speech: 93.9% confidence.

Time: 2.00
speech: 94.4% confidence.

Time: 2.50
speech: 83.7% confidence.

Time: 3.00
speech: 85.3% confidence.

Time: 3.50
speech: 94.7% confidence.

Time: 4.00
speech: 92.8% confidence.

Time: 4.50
speech: 95.1% confidence.

The request completed successfully!
```
素晴らしい、正しく`speech`（話し声）として識別されました！

### （実行例その2）ホワイトノイズの識別

次はホワイトノイズがどのように識別されるか試してみましょう。

音声ファイルの生成にはsoxコマンドが便利です。Homebrewでインストールできます。

```console
$ brew install sox
```

それでは、ホワイトノイズが10秒間記録された音声ファイルを作成します。フォーマットは44.1 kHz / 2 ch / 16 bit PCMとします。

```console
$ sox -n -r 44100 -c 2 -b 16 Noise.wav synth noise trim 0 10
```

以下のような結果が得られます。

```console
$ swift main.swift Noise.wav
Time: 0.00
waterfall: 43.1% confidence.

Time: 0.50
waterfall: 39.7% confidence.

Time: 1.00
waterfall: 47.3% confidence.

Time: 1.50
waterfall: 54.9% confidence.

Time: 2.00
waterfall: 50.9% confidence.

...

The request completed successfully!
```

`waterfall`（滝の音）として識別されました。たしかにホワイトノイズの「サー」という音色は水が流れ落ちる音に似ているので、悪くない結果です。

## （おまけ）世代別iPhoneのFLOPS比較

Apple製品にはANE（Apple Neural Engine）とよばれる機械学習のためのハードウェアアクセラレータが搭載されています。Core MLを利用する際はANEの性能を踏まえた検討が必要になります。

参考までに、iPhoneに搭載されたANEのFLOPS性能を示します。以下は[Apple Machine Learning Research](https://machinelearning.apple.com/research/neural-engine-transformers)からの引用です。

| 機種名 | チップの世代 | FLOPS |
|:---|:---|:---|
| iPhone 13 Pro | A15 | 15.8 TFlops |
| iPhone 12 Pro | A14 | 11.66 TFlops |
| iPhone 11 Pro | A13 | 5.4 TFlops |
| iPhone XS | A12 |  5.4 TFlops |
\ iPhone X | A11 |  0.6 TFlops |

## 参考資料

1. [Classifying Sounds in an Audio File - Apple Developer](https://developer.apple.com/documentation/soundanalysis/classifying_sounds_in_an_audio_file)
2. [SNClassifySoundRequest - Apple Developer](https://developer.apple.com/documentation/soundanalysis/snclassifysoundrequest)
3. [SNAudioFileAnalyzer - Apple Developer](https://developer.apple.com/documentation/soundanalysis/snaudiofileanalyzer)
4. [On-device APIs - Apple Developer](https://developer.apple.com/machine-learning/api/)
5. [Core ML - Apple Developer](https://developer.apple.com/documentation/coreml)
6. [Deploying Transformers on the Apple Neural Engine - Apple Machine Learning Research](https://machinelearning.apple.com/research/neural-engine-transformers)
